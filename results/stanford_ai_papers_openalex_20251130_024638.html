<!DOCTYPE html><html lang='en'><head><meta charset='utf-8' /><title>Stanford AI Research Dashboard</title>
    <style>
    * { box-sizing: border-box; }
    body {
        font-family: system-ui, -apple-system, BlinkMacSystemFont, "SF Pro Text", "Helvetica Neue", Arial, sans-serif;
        margin: 0;
        padding: 0;
        background: #f5f7fb;
        color: #111827;
    }
    header {
        background: linear-gradient(135deg, #1e3a8a, #0f766e);
        color: #f9fafb;
        padding: 1.75rem 2rem;
        box-shadow: 0 4px 14px rgba(15, 23, 42, 0.35);
        position: sticky;
        top: 0;
        z-index: 10;
    }
    header h1 {
        margin: 0;
        font-size: 1.6rem;
        letter-spacing: 0.02em;
    }
    .subtitle {
        margin-top: 0.5rem;
        font-size: 0.95rem;
        opacity: 0.9;
    }
    main {
        padding: 1.5rem 2rem 2.5rem;
        max-width: 1200px;
        margin: 0 auto;
    }
    .lab-filter-bar {
        margin-bottom: 1rem;
        display: flex;
        flex-wrap: wrap;
        gap: 0.5rem;
        align-items: center;
    }
    .lab-filter-title {
        font-weight: 600;
        margin-right: 0.5rem;
        font-size: 0.95rem;
    }
    .lab-chip {
        border-radius: 999px;
        border: 1px solid rgba(148, 163, 184, 0.7);
        padding: 0.25rem 0.75rem;
        background: #f9fafb;
        font-size: 0.85rem;
        cursor: pointer;
        transition: all 0.15s ease;
    }
    .lab-chip:hover {
        background: #e5e7eb;
    }
    .lab-chip.active {
        background: #1d4ed8;
        border-color: #1d4ed8;
        color: #f9fafb;
        box-shadow: 0 0 0 1px rgba(191, 219, 254, 0.9);
    }
    .lab-section {
        background: #ffffff;
        border-radius: 14px;
        padding: 1.25rem 1.25rem 1.5rem;
        margin-bottom: 1.25rem;
        box-shadow: 0 5px 18px rgba(15, 23, 42, 0.08);
    }
    .lab-title {
        margin: 0 0 0.75rem 0;
        font-size: 1.2rem;
        color: #111827;
    }
    .lab-people-grid {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
        gap: 1rem;
    }
    .person-card {
        border-radius: 12px;
        border: 1px solid #e5e7eb;
        padding: 0.75rem 0.85rem 0.9rem;
        background: #f9fafb;
    }
    .person-header {
        display: flex;
        justify-content: space-between;
        align-items: baseline;
        gap: 0.5rem;
    }
    .person-name {
        margin: 0;
        font-size: 1rem;
        font-weight: 600;
    }
    .person-name a {
        color: #1e3a8a;
        text-decoration: none;
    }
    .person-name a:hover {
        text-decoration: underline;
    }
    .person-status {
        font-size: 0.8rem;
        padding: 0.1rem 0.4rem;
        border-radius: 999px;
        background: #eef2ff;
        color: #4338ca;
        white-space: nowrap;
    }
    .person-topics {
        margin-top: 0.3rem;
        margin-bottom: 0.5rem;
    }
    .topic-badge {
        display: inline-block;
        font-size: 0.72rem;
        padding: 0.1rem 0.4rem;
        border-radius: 999px;
        background: #e0f2fe;
        color: #075985;
        margin-right: 0.25rem;
        margin-bottom: 0.15rem;
    }
    .works-list {
        list-style: none;
        padding-left: 0;
        margin: 0.25rem 0 0 0;
    }
    .work-item {
        margin-bottom: 0.5rem;
        padding-bottom: 0.4rem;
        border-bottom: 1px dashed #e5e7eb;
    }
    .work-item:last-child {
        border-bottom: none;
        margin-bottom: 0;
        padding-bottom: 0;
    }
    .work-title {
        font-size: 0.9rem;
        font-weight: 500;
        color: #111827;
        text-decoration: none;
    }
    .work-title:hover {
        text-decoration: underline;
        color: #1e3a8a;
    }
    .work-meta {
        font-size: 0.78rem;
        color: #6b7280;
        margin-top: 0.15rem;
    }
    .doi {
        font-size: 0.75rem;
        color: #4b5563;
    }
    .doi a {
        color: #1d4ed8;
        text-decoration: none;
    }
    .doi a:hover {
        text-decoration: underline;
    }
    .oa-badge {
        margin-left: 0.35rem;
        font-size: 0.7rem;
        padding: 0.08rem 0.4rem;
        border-radius: 999px;
        border: 1px solid rgba(16, 185, 129, 0.3);
        color: #047857;
        background: #ecfdf5;
        text-transform: uppercase;
    }
    .no-works {
        font-size: 0.82rem;
        color: #6b7280;
        margin: 0.35rem 0 0 0;
    }
    footer {
        text-align: center;
        font-size: 0.8rem;
        color: #6b7280;
        padding: 1rem 0 1.5rem;
    }
    @media (max-width: 640px) {
        header {
            padding: 1.25rem 1rem;
        }
        main {
        padding: 1rem 1rem 2rem;
        }
    }
    </style>
    
    <script>
    function filterByLab(labId) {
        const sections = document.querySelectorAll('.lab-section');
        const chips = document.querySelectorAll('.lab-chip');

        if (!labId) {
            sections.forEach(s => s.style.display = '');
            chips.forEach(c => c.classList.remove('active'));
            return;
        }

        sections.forEach(sec => {
            if (sec.getAttribute('data-lab') === labId) {
                sec.style.display = '';
            } else {
                sec.style.display = 'none';
            }
        });

        chips.forEach(c => {
            if (c.getAttribute('data-lab') === labId) {
                c.classList.add('active');
            } else {
                c.classList.remove('active');
            }
        });
    }
    </script>
    </head><body><header><h1>Stanford AI Research Dashboard</h1><div class='subtitle'>Data source: OpenAlex · From date: 2025-06-01 · Generated at: 2025-11-30T02:46:57.152066Z</div></header><main><div class='lab-filter-bar'><span class='lab-filter-title'>Filter by lab:</span><button class='lab-chip' onclick="filterByLab('')">All</button><button class='lab-chip' data-lab='CRFM' onclick="filterByLab('CRFM')">CRFM</button><button class='lab-chip' data-lab='HAI' onclick="filterByLab('HAI')">HAI</button><button class='lab-chip' data-lab='IRIS' onclick="filterByLab('IRIS')">IRIS</button><button class='lab-chip' data-lab='ML_Group' onclick="filterByLab('ML_Group')">ML Group</button><button class='lab-chip' data-lab='SAIL' onclick="filterByLab('SAIL')">SAIL</button><button class='lab-chip' data-lab='Stanford_NLP_Group' onclick="filterByLab('Stanford_NLP_Group')">Stanford NLP Group</button><button class='lab-chip' data-lab='Vision__amp;_Learning_Lab' onclick="filterByLab('Vision__amp;_Learning_Lab')">Vision &amp; Learning Lab</button></div><section class='lab-section' id='lab-CRFM' data-lab='CRFM'><h2 class='lab-title'>CRFM</h2><div class='lab-people-grid'><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://cs.stanford.edu/~pliang/' target='_blank'>Percy Liang</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>foundation models</span> <span class='topic-badge'>llm</span> <span class='topic-badge'>alignment</span> <span class='topic-badge'>evaluation</span> <span class='topic-badge'>program synthesis</span></div><ul class='works-list'><li class='work-item'><a href='http://arxiv.org/abs/2510.11977' target='_blank' class='work-title'>Holistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-13 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.11977' target='_blank'>https://doi.org/10.48550/arxiv.2510.11977</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2510.07307' target='_blank' class='work-title'>MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-08 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.07307' target='_blank'>https://doi.org/10.48550/arxiv.2510.07307</a></span></li><li class='work-item'><a href='https://doi.org/10.58530/2025/3377' target='_blank' class='work-title'>Using Large Language Models and Retrieval-Augmented Generation in MRI Protocol Selection: Balancing Accuracy and Privacy</a><div class='work-meta'>2025 · Proceedings on CD-ROM - International Society for Magnetic Resonance in Medicine. Scientific Meeting and Exhibition/Proceedings of the International Society for Magnetic Resonance in Medicine, Scientific Meeting and Exhibition · 2025-09-16 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.58530/2025/3377' target='_blank'>https://doi.org/10.58530/2025/3377</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2509.05627' target='_blank' class='work-title'>Audits Under Resource, Data, and Access Constraints: Scaling Laws For Less Discriminatory Alternatives</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-09-06 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2509.05627' target='_blank'>https://doi.org/10.48550/arxiv.2509.05627</a></span></li><li class='work-item'><a href='https://doi.org/10.1126/science.adu8449' target='_blank' class='work-title'>Advancing science- and evidence-based AI policy</a><div class='work-meta'>2025 · Science · 2025-07-31 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1126/science.adu8449' target='_blank'>https://doi.org/10.1126/science.adu8449</a></span></li><li class='work-item'><a href='https://doi.org/10.15607/rss.2025.xxi.017' target='_blank' class='work-title'>Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success</a><div class='work-meta'>2025 · 2025-06-21 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.15607/rss.2025.xxi.017' target='_blank'>https://doi.org/10.15607/rss.2025.xxi.017</a></span></li></ul></div><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://rishibommasani.github.io/' target='_blank'>Rishi Bommasani</a></h3><span class='person-status'>Researcher</span></div><div class='person-topics'><span class='topic-badge'>foundation models</span> <span class='topic-badge'>evaluation</span> <span class='topic-badge'>policy</span> <span class='topic-badge'>societal impact</span></div><ul class='works-list'><li class='work-item'><a href='https://doi.org/10.1609/aies.v8i3.36700' target='_blank' class='work-title'>Disclosure and Evaluation as Fairness Interventions for General-Purpose AI</a><div class='work-meta'>2025 · Proceedings of the AAAI/ACM Conference on AI Ethics and Society · 2025-10-15 <span class='oa-badge oa-bronze'>bronze</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1609/aies.v8i3.36700' target='_blank'>https://doi.org/10.1609/aies.v8i3.36700</a></span></li><li class='work-item'><a href='https://doi.org/10.1609/aies.v8i3.36743' target='_blank' class='work-title'>Do AI Companies Make Good on Voluntary Commitments to the White House?</a><div class='work-meta'>2025 · Proceedings of the AAAI/ACM Conference on AI Ethics and Society · 2025-10-15 <span class='oa-badge oa-bronze'>bronze</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1609/aies.v8i3.36743' target='_blank'>https://doi.org/10.1609/aies.v8i3.36743</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2510.11977' target='_blank' class='work-title'>Holistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-13 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.11977' target='_blank'>https://doi.org/10.48550/arxiv.2510.11977</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2510.05292' target='_blank' class='work-title'>Disclosure and Evaluation as Fairness Interventions for General-Purpose AI</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-06 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.05292' target='_blank'>https://doi.org/10.48550/arxiv.2510.05292</a></span></li><li class='work-item'><a href='https://doi.org/10.1126/science.adu8449' target='_blank' class='work-title'>Advancing science- and evidence-based AI policy</a><div class='work-meta'>2025 · Science · 2025-07-31 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1126/science.adu8449' target='_blank'>https://doi.org/10.1126/science.adu8449</a></span></li><li class='work-item'><a href='https://doi.org/10.1145/3715275.3732048' target='_blank' class='work-title'>The Reality of AI and Biorisk</a><div class='work-meta'>2025 · 2025-06-23 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1145/3715275.3732048' target='_blank'>https://doi.org/10.1145/3715275.3732048</a></span></li></ul></div></div></section><section class='lab-section' id='lab-HAI' data-lab='HAI'><h2 class='lab-title'>HAI</h2><div class='lab-people-grid'><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://profiles.stanford.edu/christopher-potts' target='_blank'>Chris Potts</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>nlp</span> <span class='topic-badge'>semantics</span> <span class='topic-badge'>pragmatics</span> <span class='topic-badge'>safety</span></div><ul class='works-list'><li class='work-item'><a href='https://doi.org/10.1016/j.envpol.2025.127228' target='_blank' class='work-title'>Polystyrene and polyethylene terephthalate nanoplastics differentially impact mouse ovarian follicle function</a><div class='work-meta'>2025 · Environmental Pollution · 2025-10-08 <span class='oa-badge oa-hybrid'>hybrid</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1016/j.envpol.2025.127228' target='_blank'>https://doi.org/10.1016/j.envpol.2025.127228</a></span></li></ul></div><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://nlp.stanford.edu/~manning/' target='_blank'>Christopher D. Manning</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>nlp</span> <span class='topic-badge'>llm</span> <span class='topic-badge'>representation learning</span> <span class='topic-badge'>syntax</span> <span class='topic-badge'>semantics</span></div><ul class='works-list'><li class='work-item'><a href='http://arxiv.org/abs/2510.02569' target='_blank' class='work-title'>Transcribe, Translate, or Transliterate: An Investigation of Intermediate Representations in Spoken Language Models</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-02 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.02569' target='_blank'>https://doi.org/10.48550/arxiv.2510.02569</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2510.01171' target='_blank' class='work-title'>Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-01 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.01171' target='_blank'>https://doi.org/10.48550/arxiv.2510.01171</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2510.00219' target='_blank' class='work-title'>Thoughtbubbles: an Unsupervised Method for Parallel Thinking in Latent Space</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-09-30 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.00219' target='_blank'>https://doi.org/10.48550/arxiv.2510.00219</a></span></li><li class='work-item'><a href='https://doi.org/10.1016/j.jim.2025.113970' target='_blank' class='work-title'>CAR binding efficacy and off-target interactions for common CAR detection reagents</a><div class='work-meta'>2025 · Journal of Immunological Methods · 2025-08-28 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1016/j.jim.2025.113970' target='_blank'>https://doi.org/10.1016/j.jim.2025.113970</a></span></li><li class='work-item'><a href='https://doi.org/10.1038/s41562-025-02273-8' target='_blank' class='work-title'>Quantifying large language model usage in scientific papers</a><div class='work-meta'>2025 · Nature Human Behaviour · 2025-08-04 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1038/s41562-025-02273-8' target='_blank'>https://doi.org/10.1038/s41562-025-02273-8</a></span></li><li class='work-item'><a href='https://doi.org/10.1145/3711896.3736801' target='_blank' class='work-title'>The Surprising Victory of NLP: From Philosophy to Agentic Language Models</a><div class='work-meta'>2025 · 2025-08-01 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1145/3711896.3736801' target='_blank'>https://doi.org/10.1145/3711896.3736801</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2507.18103' target='_blank' class='work-title'>A New Pair of GloVes</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-07-24 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2507.18103' target='_blank'>https://doi.org/10.48550/arxiv.2507.18103</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2506.16678' target='_blank' class='work-title'>Mechanisms vs. Outcomes: Probing for Syntax Fails to Explain Performance on Targeted Syntactic Evaluations</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-06-20 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2506.16678' target='_blank'>https://doi.org/10.48550/arxiv.2506.16678</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2506.03989' target='_blank' class='work-title'>Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-06-04 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2506.03989' target='_blank'>https://doi.org/10.48550/arxiv.2506.03989</a></span></li></ul></div><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://cs.stanford.edu/people/chrismre/' target='_blank'>Christopher Ré</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>ml systems</span> <span class='topic-badge'>weak supervision</span> <span class='topic-badge'>foundation models</span></div><ul class='works-list'><li class='work-item'><a href='https://doi.org/10.1109/cvpr52734.2025.00242' target='_blank' class='work-title'>HMAR: Efficient Hierarchical Masked Auto-Regressive Image Generation</a><div class='work-meta'>2025 · 2025-06-10 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1109/cvpr52734.2025.00242' target='_blank'>https://doi.org/10.1109/cvpr52734.2025.00242</a></span></li></ul></div><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://cs.stanford.edu/~diyiy/' target='_blank'>Diyi Yang</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>nlp</span> <span class='topic-badge'>social nlp</span> <span class='topic-badge'>evaluation</span> <span class='topic-badge'>safety</span></div><ul class='works-list'><li class='work-item'><a href='http://arxiv.org/abs/2510.27672' target='_blank' class='work-title'>Culture Cartography: Mapping the Landscape of Cultural Knowledge</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-31 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.27672' target='_blank'>https://doi.org/10.48550/arxiv.2510.27672</a></span></li><li class='work-item'><a href='https://doi.org/10.5210/fm.v30i8.13877' target='_blank' class='work-title'>Understanding #vent channels on Discord</a><div class='work-meta'>2025 · First Monday · 2025-08-17 <span class='oa-badge oa-diamond'>diamond</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.5210/fm.v30i8.13877' target='_blank'>https://doi.org/10.5210/fm.v30i8.13877</a></span></li><li class='work-item'><a href='https://doi.org/10.1038/s41562-025-02273-8' target='_blank' class='work-title'>Quantifying large language model usage in scientific papers</a><div class='work-meta'>2025 · Nature Human Behaviour · 2025-08-04 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1038/s41562-025-02273-8' target='_blank'>https://doi.org/10.1038/s41562-025-02273-8</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2507.12705' target='_blank' class='work-title'>AudioJudge: Understanding What Works in Large Audio Model Based Speech Evaluation</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-07-17 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2507.12705' target='_blank'>https://doi.org/10.48550/arxiv.2507.12705</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2507.03041' target='_blank' class='work-title'>Optimas: Optimizing Compound AI Systems with Globally Aligned Local Rewards</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-07-03 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2507.03041' target='_blank'>https://doi.org/10.48550/arxiv.2507.03041</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2506.20803' target='_blank' class='work-title'>The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-06-25 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2506.20803' target='_blank'>https://doi.org/10.48550/arxiv.2506.20803</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2506.12605' target='_blank' class='work-title'>The Rise of AI Companions: How Human-Chatbot Relationships Influence Well-Being</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-06-14 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2506.12605' target='_blank'>https://doi.org/10.48550/arxiv.2506.12605</a></span></li></ul></div><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://profiles.stanford.edu/fei-fei-li' target='_blank'>Fei-Fei Li</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>computer vision</span> <span class='topic-badge'>multimodal</span> <span class='topic-badge'>robotics</span> <span class='topic-badge'>healthcare</span></div><ul class='works-list'><li class='work-item'><a href='http://arxiv.org/abs/2510.18316' target='_blank' class='work-title'>MoMaGen: Generating Demonstrations under Soft and Hard Constraints for Multi-Step Bimanual Mobile Manipulation</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-21 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.18316' target='_blank'>https://doi.org/10.48550/arxiv.2510.18316</a></span></li><li class='work-item'><a href='https://doi.org/10.21203/rs.3.rs-7359180/v1' target='_blank' class='work-title'>EEG-Based Brain-Computer Interface for Robotic Assistance with User Intention Prediction</a><div class='work-meta'>2025 · 2025-09-19 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.21203/rs.3.rs-7359180/v1' target='_blank'>https://doi.org/10.21203/rs.3.rs-7359180/v1</a></span></li><li class='work-item'><a href='https://doi.org/10.1126/science.adu8449' target='_blank' class='work-title'>Advancing science- and evidence-based AI policy</a><div class='work-meta'>2025 · Science · 2025-07-31 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1126/science.adu8449' target='_blank'>https://doi.org/10.1126/science.adu8449</a></span></li><li class='work-item'><a href='https://doi.org/10.1109/cvprw67362.2025.00350' target='_blank' class='work-title'>An Interactive Agent Foundation Model</a><div class='work-meta'>2025 · 2025-06-11 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1109/cvprw67362.2025.00350' target='_blank'>https://doi.org/10.1109/cvprw67362.2025.00350</a></span></li><li class='work-item'><a href='https://doi.org/10.1109/cvpr52734.2025.00802' target='_blank' class='work-title'>Re-thinking Temporal Search for Long-Form Video Understanding</a><div class='work-meta'>2025 · 2025-06-10 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1109/cvpr52734.2025.00802' target='_blank'>https://doi.org/10.1109/cvpr52734.2025.00802</a></span></li><li class='work-item'><a href='https://doi.org/10.1109/cvpr52734.2025.00994' target='_blank' class='work-title'>Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces</a><div class='work-meta'>2025 · 2025-06-10 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1109/cvpr52734.2025.00994' target='_blank'>https://doi.org/10.1109/cvpr52734.2025.00994</a></span></li><li class='work-item'><a href='https://doi.org/10.1109/cvpr52734.2025.00581' target='_blank' class='work-title'>The Language of Motion: Unifying Verbal and Non-verbal Language of 3D Human Motion</a><div class='work-meta'>2025 · 2025-06-10 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1109/cvpr52734.2025.00581' target='_blank'>https://doi.org/10.1109/cvpr52734.2025.00581</a></span></li><li class='work-item'><a href='https://doi.org/10.1038/s41746-025-01730-y' target='_blank' class='work-title'>Foundation versus domain-specific models for left ventricular segmentation on cardiac ultrasound</a><div class='work-meta'>2025 · npj Digital Medicine · 2025-06-06 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1038/s41746-025-01730-y' target='_blank'>https://doi.org/10.1038/s41746-025-01730-y</a></span></li></ul></div><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://cs.stanford.edu/~pliang/' target='_blank'>Percy Liang</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>foundation models</span> <span class='topic-badge'>llm</span> <span class='topic-badge'>alignment</span> <span class='topic-badge'>evaluation</span> <span class='topic-badge'>program synthesis</span></div><ul class='works-list'><li class='work-item'><a href='http://arxiv.org/abs/2510.11977' target='_blank' class='work-title'>Holistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-13 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.11977' target='_blank'>https://doi.org/10.48550/arxiv.2510.11977</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2510.07307' target='_blank' class='work-title'>MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-08 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.07307' target='_blank'>https://doi.org/10.48550/arxiv.2510.07307</a></span></li><li class='work-item'><a href='https://doi.org/10.58530/2025/3377' target='_blank' class='work-title'>Using Large Language Models and Retrieval-Augmented Generation in MRI Protocol Selection: Balancing Accuracy and Privacy</a><div class='work-meta'>2025 · Proceedings on CD-ROM - International Society for Magnetic Resonance in Medicine. Scientific Meeting and Exhibition/Proceedings of the International Society for Magnetic Resonance in Medicine, Scientific Meeting and Exhibition · 2025-09-16 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.58530/2025/3377' target='_blank'>https://doi.org/10.58530/2025/3377</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2509.05627' target='_blank' class='work-title'>Audits Under Resource, Data, and Access Constraints: Scaling Laws For Less Discriminatory Alternatives</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-09-06 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2509.05627' target='_blank'>https://doi.org/10.48550/arxiv.2509.05627</a></span></li><li class='work-item'><a href='https://doi.org/10.1126/science.adu8449' target='_blank' class='work-title'>Advancing science- and evidence-based AI policy</a><div class='work-meta'>2025 · Science · 2025-07-31 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1126/science.adu8449' target='_blank'>https://doi.org/10.1126/science.adu8449</a></span></li><li class='work-item'><a href='https://doi.org/10.15607/rss.2025.xxi.017' target='_blank' class='work-title'>Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success</a><div class='work-meta'>2025 · 2025-06-21 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.15607/rss.2025.xxi.017' target='_blank'>https://doi.org/10.15607/rss.2025.xxi.017</a></span></li></ul></div><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://rishibommasani.github.io/' target='_blank'>Rishi Bommasani</a></h3><span class='person-status'>Researcher</span></div><div class='person-topics'><span class='topic-badge'>foundation models</span> <span class='topic-badge'>evaluation</span> <span class='topic-badge'>policy</span> <span class='topic-badge'>societal impact</span></div><ul class='works-list'><li class='work-item'><a href='https://doi.org/10.1609/aies.v8i3.36700' target='_blank' class='work-title'>Disclosure and Evaluation as Fairness Interventions for General-Purpose AI</a><div class='work-meta'>2025 · Proceedings of the AAAI/ACM Conference on AI Ethics and Society · 2025-10-15 <span class='oa-badge oa-bronze'>bronze</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1609/aies.v8i3.36700' target='_blank'>https://doi.org/10.1609/aies.v8i3.36700</a></span></li><li class='work-item'><a href='https://doi.org/10.1609/aies.v8i3.36743' target='_blank' class='work-title'>Do AI Companies Make Good on Voluntary Commitments to the White House?</a><div class='work-meta'>2025 · Proceedings of the AAAI/ACM Conference on AI Ethics and Society · 2025-10-15 <span class='oa-badge oa-bronze'>bronze</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1609/aies.v8i3.36743' target='_blank'>https://doi.org/10.1609/aies.v8i3.36743</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2510.11977' target='_blank' class='work-title'>Holistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-13 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.11977' target='_blank'>https://doi.org/10.48550/arxiv.2510.11977</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2510.05292' target='_blank' class='work-title'>Disclosure and Evaluation as Fairness Interventions for General-Purpose AI</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-06 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.05292' target='_blank'>https://doi.org/10.48550/arxiv.2510.05292</a></span></li><li class='work-item'><a href='https://doi.org/10.1126/science.adu8449' target='_blank' class='work-title'>Advancing science- and evidence-based AI policy</a><div class='work-meta'>2025 · Science · 2025-07-31 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1126/science.adu8449' target='_blank'>https://doi.org/10.1126/science.adu8449</a></span></li><li class='work-item'><a href='https://doi.org/10.1145/3715275.3732048' target='_blank' class='work-title'>The Reality of AI and Biorisk</a><div class='work-meta'>2025 · 2025-06-23 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1145/3715275.3732048' target='_blank'>https://doi.org/10.1145/3715275.3732048</a></span></li></ul></div><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://thashim.github.io/' target='_blank'>Tatsunori Hashimoto</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>llm</span> <span class='topic-badge'>evaluation</span> <span class='topic-badge'>robustness</span> <span class='topic-badge'>alignment</span> <span class='topic-badge'>statistics</span></div><ul class='works-list'><li class='work-item'><a href='https://doi.org/10.1093/oxfordhb/9780198940272.013.0021' target='_blank' class='work-title'>Liability for AI-Generated Speech</a><div class='work-meta'>2025 · Oxford University Press eBooks · 2025-07-22 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1093/oxfordhb/9780198940272.013.0021' target='_blank'>https://doi.org/10.1093/oxfordhb/9780198940272.013.0021</a></span></li><li class='work-item'><a href='https://doi.org/10.1038/s42256-025-01038-2' target='_blank' class='work-title'>The future of open human feedback</a><div class='work-meta'>2025 · Nature Machine Intelligence · 2025-06-20 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1038/s42256-025-01038-2' target='_blank'>https://doi.org/10.1038/s42256-025-01038-2</a></span></li></ul></div></div></section><section class='lab-section' id='lab-IRIS' data-lab='IRIS'><h2 class='lab-title'>IRIS</h2><div class='lab-people-grid'><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://ai.stanford.edu/~cbfinn/' target='_blank'>Chelsea Finn</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>robotics</span> <span class='topic-badge'>rl</span> <span class='topic-badge'>meta-learning</span> <span class='topic-badge'>embodied ai</span></div><ul class='works-list'><li class='work-item'><a href='http://arxiv.org/abs/2510.10125' target='_blank' class='work-title'>Ctrl-World: A Controllable Generative World Model for Robot Manipulation</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-11 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.10125' target='_blank'>https://doi.org/10.48550/arxiv.2510.10125</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2510.02263' target='_blank' class='work-title'>RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-02 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.02263' target='_blank'>https://doi.org/10.48550/arxiv.2510.02263</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2507.07986' target='_blank' class='work-title'>EXPO: Stable Reinforcement Learning with Expressive Policies</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-07-10 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2507.07986' target='_blank'>https://doi.org/10.48550/arxiv.2507.07986</a></span></li><li class='work-item'><a href='https://doi.org/10.1126/scirobotics.adt5254' target='_blank' class='work-title'>SRT-H: A hierarchical framework for autonomous surgery via language-conditioned imitation learning</a><div class='work-meta'>2025 · Science Robotics · 2025-07-09 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1126/scirobotics.adt5254' target='_blank'>https://doi.org/10.1126/scirobotics.adt5254</a></span></li><li class='work-item'><a href='https://doi.org/10.15607/rss.2025.xxi.010' target='_blank' class='work-title'>π₀: A Vision-Language-Action Flow Model for General Robot Control</a><div class='work-meta'>2025 · 2025-06-21 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.15607/rss.2025.xxi.010' target='_blank'>https://doi.org/10.15607/rss.2025.xxi.010</a></span></li><li class='work-item'><a href='https://doi.org/10.15607/rss.2025.xxi.012' target='_blank' class='work-title'>FAST: Efficient Action Tokenization for Vision-Language-Action Models</a><div class='work-meta'>2025 · 2025-06-21 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.15607/rss.2025.xxi.012' target='_blank'>https://doi.org/10.15607/rss.2025.xxi.012</a></span></li><li class='work-item'><a href='https://doi.org/10.15607/rss.2025.xxi.071' target='_blank' class='work-title'>Curating Demonstrations using Online Experience</a><div class='work-meta'>2025 · 2025-06-21 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.15607/rss.2025.xxi.071' target='_blank'>https://doi.org/10.15607/rss.2025.xxi.071</a></span></li><li class='work-item'><a href='https://doi.org/10.15607/rss.2025.xxi.017' target='_blank' class='work-title'>Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success</a><div class='work-meta'>2025 · 2025-06-21 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.15607/rss.2025.xxi.017' target='_blank'>https://doi.org/10.15607/rss.2025.xxi.017</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2505.10251' target='_blank' class='work-title'>Hierarchical Surgical Robot Transformer (SRT-H): Imitation Learning for Autonomous Surgery</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-06-10 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.5281/zenodo.15637073' target='_blank'>https://doi.org/10.5281/zenodo.15637073</a></span></li><li class='work-item'><a href='https://doi.org/10.1109/cvpr52734.2025.00166' target='_blank' class='work-title'>CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models</a><div class='work-meta'>2025 · 2025-06-10 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1109/cvpr52734.2025.00166' target='_blank'>https://doi.org/10.1109/cvpr52734.2025.00166</a></span></li></ul></div><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://dorsa.fyi/' target='_blank'>Dorsa Sadigh</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>robotics</span> <span class='topic-badge'>human-robot interaction</span> <span class='topic-badge'>rl</span> <span class='topic-badge'>safety</span></div><ul class='works-list'><li class='work-item'><a href='http://arxiv.org/abs/2507.07986' target='_blank' class='work-title'>EXPO: Stable Reinforcement Learning with Expressive Policies</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-07-10 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2507.07986' target='_blank'>https://doi.org/10.48550/arxiv.2507.07986</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2506.19212' target='_blank' class='work-title'>Scaffolding Dexterous Manipulation with Vision-Language Models</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-06-24 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2506.19212' target='_blank'>https://doi.org/10.48550/arxiv.2506.19212</a></span></li><li class='work-item'><a href='https://doi.org/10.15607/rss.2025.xxi.155' target='_blank' class='work-title'>Bridging Perception and Action: Spatially-Grounded Mid-Level Representations for Robot Generalization</a><div class='work-meta'>2025 · 2025-06-21 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.15607/rss.2025.xxi.155' target='_blank'>https://doi.org/10.15607/rss.2025.xxi.155</a></span></li><li class='work-item'><a href='https://doi.org/10.15607/rss.2025.xxi.023' target='_blank' class='work-title'>Robot Data Curation with Mutual Information Estimators</a><div class='work-meta'>2025 · 2025-06-21 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.15607/rss.2025.xxi.023' target='_blank'>https://doi.org/10.15607/rss.2025.xxi.023</a></span></li><li class='work-item'><a href='https://doi.org/10.15607/rss.2025.xxi.074' target='_blank' class='work-title'>Unified Video Action Model</a><div class='work-meta'>2025 · 2025-06-21 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.15607/rss.2025.xxi.074' target='_blank'>https://doi.org/10.15607/rss.2025.xxi.074</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2506.12248' target='_blank' class='work-title'>ProVox: Personalization and Proactive Planning for Situated Human-Robot Collaboration</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-06-13 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2506.12248' target='_blank'>https://doi.org/10.48550/arxiv.2506.12248</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2506.01185' target='_blank' class='work-title'>HoMeR: Learning In-the-Wild Mobile Manipulation via Hybrid Imitation and Whole-Body Control</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-06-01 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2506.01185' target='_blank'>https://doi.org/10.48550/arxiv.2506.01185</a></span></li></ul></div></div></section><section class='lab-section' id='lab-ML_Group' data-lab='ML_Group'><h2 class='lab-title'>ML Group</h2><div class='lab-people-grid'><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://stanfordmlgroup.github.io/' target='_blank'>Andrew Ng</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>machine learning</span> <span class='topic-badge'>applied ml</span> <span class='topic-badge'>healthcare</span> <span class='topic-badge'>representation learning</span></div><ul class='works-list'><li class='work-item'><a href='http://arxiv.org/abs/2511.00383' target='_blank' class='work-title'>STARC-9: A Large-scale Dataset for Multi-Class Tissue Classification for CRC Histopathology</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-11-01 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2511.00383' target='_blank'>https://doi.org/10.48550/arxiv.2511.00383</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2510.01876' target='_blank' class='work-title'>Quasi-convex surface subgroups in some one-relator groups with torsion</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-02 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.01876' target='_blank'>https://doi.org/10.48550/arxiv.2510.01876</a></span></li><li class='work-item'><a href='https://doi.org/10.1016/j.jenvman.2025.126728' target='_blank' class='work-title'>Regional mapping of natural gas compressor stations in the United States and Canada using deep learning on satellite imagery</a><div class='work-meta'>2025 · Journal of Environmental Management · 2025-08-29 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1016/j.jenvman.2025.126728' target='_blank'>https://doi.org/10.1016/j.jenvman.2025.126728</a></span></li><li class='work-item'><a href='https://doi.org/10.1056/aidbp2500144' target='_blank' class='work-title'>MedAgentBench: A Virtual EHR Environment to Benchmark Medical LLM Agents</a><div class='work-meta'>2025 · NEJM AI · 2025-08-14 <span class='oa-badge oa-bronze'>bronze</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1056/aidbp2500144' target='_blank'>https://doi.org/10.1056/aidbp2500144</a></span></li></ul></div><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://ai.stanford.edu/~cbfinn/' target='_blank'>Chelsea Finn</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>robotics</span> <span class='topic-badge'>rl</span> <span class='topic-badge'>meta-learning</span> <span class='topic-badge'>embodied ai</span></div><ul class='works-list'><li class='work-item'><a href='http://arxiv.org/abs/2510.10125' target='_blank' class='work-title'>Ctrl-World: A Controllable Generative World Model for Robot Manipulation</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-11 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.10125' target='_blank'>https://doi.org/10.48550/arxiv.2510.10125</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2510.02263' target='_blank' class='work-title'>RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-02 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.02263' target='_blank'>https://doi.org/10.48550/arxiv.2510.02263</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2507.07986' target='_blank' class='work-title'>EXPO: Stable Reinforcement Learning with Expressive Policies</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-07-10 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2507.07986' target='_blank'>https://doi.org/10.48550/arxiv.2507.07986</a></span></li><li class='work-item'><a href='https://doi.org/10.1126/scirobotics.adt5254' target='_blank' class='work-title'>SRT-H: A hierarchical framework for autonomous surgery via language-conditioned imitation learning</a><div class='work-meta'>2025 · Science Robotics · 2025-07-09 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1126/scirobotics.adt5254' target='_blank'>https://doi.org/10.1126/scirobotics.adt5254</a></span></li><li class='work-item'><a href='https://doi.org/10.15607/rss.2025.xxi.010' target='_blank' class='work-title'>π₀: A Vision-Language-Action Flow Model for General Robot Control</a><div class='work-meta'>2025 · 2025-06-21 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.15607/rss.2025.xxi.010' target='_blank'>https://doi.org/10.15607/rss.2025.xxi.010</a></span></li><li class='work-item'><a href='https://doi.org/10.15607/rss.2025.xxi.012' target='_blank' class='work-title'>FAST: Efficient Action Tokenization for Vision-Language-Action Models</a><div class='work-meta'>2025 · 2025-06-21 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.15607/rss.2025.xxi.012' target='_blank'>https://doi.org/10.15607/rss.2025.xxi.012</a></span></li><li class='work-item'><a href='https://doi.org/10.15607/rss.2025.xxi.071' target='_blank' class='work-title'>Curating Demonstrations using Online Experience</a><div class='work-meta'>2025 · 2025-06-21 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.15607/rss.2025.xxi.071' target='_blank'>https://doi.org/10.15607/rss.2025.xxi.071</a></span></li><li class='work-item'><a href='https://doi.org/10.15607/rss.2025.xxi.017' target='_blank' class='work-title'>Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success</a><div class='work-meta'>2025 · 2025-06-21 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.15607/rss.2025.xxi.017' target='_blank'>https://doi.org/10.15607/rss.2025.xxi.017</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2505.10251' target='_blank' class='work-title'>Hierarchical Surgical Robot Transformer (SRT-H): Imitation Learning for Autonomous Surgery</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-06-10 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.5281/zenodo.15637073' target='_blank'>https://doi.org/10.5281/zenodo.15637073</a></span></li><li class='work-item'><a href='https://doi.org/10.1109/cvpr52734.2025.00166' target='_blank' class='work-title'>CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models</a><div class='work-meta'>2025 · 2025-06-10 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1109/cvpr52734.2025.00166' target='_blank'>https://doi.org/10.1109/cvpr52734.2025.00166</a></span></li></ul></div><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://cs.stanford.edu/~jure/' target='_blank'>Jure Leskovec</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>graph learning</span> <span class='topic-badge'>recommendation</span> <span class='topic-badge'>network science</span></div><ul class='works-list'><li class='work-item'><a href='http://arxiv.org/abs/2510.06377' target='_blank' class='work-title'>Relational Transformer: Toward Zero-Shot Foundation Models for Relational Data</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-07 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.06377' target='_blank'>https://doi.org/10.48550/arxiv.2510.06377</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2509.25604' target='_blank' class='work-title'>RFG: Test-Time Scaling for Diffusion Large Language Model Reasoning with Reward-Free Guidance</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-09-29 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2509.25604' target='_blank'>https://doi.org/10.48550/arxiv.2509.25604</a></span></li><li class='work-item'><a href='https://doi.org/10.1101/2025.09.23.678086' target='_blank' class='work-title'>A deep reinforcement learning platform for antibiotic discovery</a><div class='work-meta'>2025 · bioRxiv (Cold Spring Harbor Laboratory) · 2025-09-23 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1101/2025.09.23.678086' target='_blank'>https://doi.org/10.1101/2025.09.23.678086</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2508.11800' target='_blank' class='work-title'>Uncalibrated Reasoning: GRPO Induces Overconfidence for Stochastic Outcomes</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-08-15 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2508.11800' target='_blank'>https://doi.org/10.48550/arxiv.2508.11800</a></span></li><li class='work-item'><a href='https://doi.org/10.1038/s41586-025-09321-3' target='_blank' class='work-title'>Countrywide natural experiment links built environment to physical activity</a><div class='work-meta'>2025 · Nature · 2025-08-13 <span class='oa-badge oa-hybrid'>hybrid</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1038/s41586-025-09321-3' target='_blank'>https://doi.org/10.1038/s41586-025-09321-3</a></span></li><li class='work-item'><a href='https://doi.org/10.1145/3711896.3736558' target='_blank' class='work-title'>Relational Deep Learning: Challenges, Foundations and Next-Generation Architectures</a><div class='work-meta'>2025 · 2025-08-03 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1145/3711896.3736558' target='_blank'>https://doi.org/10.1145/3711896.3736558</a></span></li><li class='work-item'><a href='https://doi.org/10.1145/3711896.3737139' target='_blank' class='work-title'>Surface-based Molecular Design with Multi-modal Flow Matching</a><div class='work-meta'>2025 · 2025-08-03 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1145/3711896.3737139' target='_blank'>https://doi.org/10.1145/3711896.3737139</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2507.16991' target='_blank' class='work-title'>PyG 2.0: Scalable Learning on Real World Graphs</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-07-22 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2507.16991' target='_blank'>https://doi.org/10.48550/arxiv.2507.16991</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2507.03041' target='_blank' class='work-title'>Optimas: Optimizing Compound AI Systems with Globally Aligned Local Rewards</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-07-03 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2507.03041' target='_blank'>https://doi.org/10.48550/arxiv.2507.03041</a></span></li><li class='work-item'><a href='https://doi.org/10.1101/2025.06.26.661135' target='_blank' class='work-title'>Predicting cellular responses to perturbation across diverse contexts with State</a><div class='work-meta'>2025 · bioRxiv (Cold Spring Harbor Laboratory) · 2025-06-27 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1101/2025.06.26.661135' target='_blank'>https://doi.org/10.1101/2025.06.26.661135</a></span></li></ul></div><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://cs.stanford.edu/~pliang/' target='_blank'>Percy Liang</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>foundation models</span> <span class='topic-badge'>llm</span> <span class='topic-badge'>alignment</span> <span class='topic-badge'>evaluation</span> <span class='topic-badge'>program synthesis</span></div><ul class='works-list'><li class='work-item'><a href='http://arxiv.org/abs/2510.11977' target='_blank' class='work-title'>Holistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-13 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.11977' target='_blank'>https://doi.org/10.48550/arxiv.2510.11977</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2510.07307' target='_blank' class='work-title'>MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-08 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.07307' target='_blank'>https://doi.org/10.48550/arxiv.2510.07307</a></span></li><li class='work-item'><a href='https://doi.org/10.58530/2025/3377' target='_blank' class='work-title'>Using Large Language Models and Retrieval-Augmented Generation in MRI Protocol Selection: Balancing Accuracy and Privacy</a><div class='work-meta'>2025 · Proceedings on CD-ROM - International Society for Magnetic Resonance in Medicine. Scientific Meeting and Exhibition/Proceedings of the International Society for Magnetic Resonance in Medicine, Scientific Meeting and Exhibition · 2025-09-16 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.58530/2025/3377' target='_blank'>https://doi.org/10.58530/2025/3377</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2509.05627' target='_blank' class='work-title'>Audits Under Resource, Data, and Access Constraints: Scaling Laws For Less Discriminatory Alternatives</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-09-06 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2509.05627' target='_blank'>https://doi.org/10.48550/arxiv.2509.05627</a></span></li><li class='work-item'><a href='https://doi.org/10.1126/science.adu8449' target='_blank' class='work-title'>Advancing science- and evidence-based AI policy</a><div class='work-meta'>2025 · Science · 2025-07-31 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1126/science.adu8449' target='_blank'>https://doi.org/10.1126/science.adu8449</a></span></li><li class='work-item'><a href='https://doi.org/10.15607/rss.2025.xxi.017' target='_blank' class='work-title'>Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success</a><div class='work-meta'>2025 · 2025-06-21 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.15607/rss.2025.xxi.017' target='_blank'>https://doi.org/10.15607/rss.2025.xxi.017</a></span></li></ul></div></div></section><section class='lab-section' id='lab-SAIL' data-lab='SAIL'><h2 class='lab-title'>SAIL</h2><div class='lab-people-grid'><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://stanfordmlgroup.github.io/' target='_blank'>Andrew Ng</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>machine learning</span> <span class='topic-badge'>applied ml</span> <span class='topic-badge'>healthcare</span> <span class='topic-badge'>representation learning</span></div><ul class='works-list'><li class='work-item'><a href='http://arxiv.org/abs/2511.00383' target='_blank' class='work-title'>STARC-9: A Large-scale Dataset for Multi-Class Tissue Classification for CRC Histopathology</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-11-01 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2511.00383' target='_blank'>https://doi.org/10.48550/arxiv.2511.00383</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2510.01876' target='_blank' class='work-title'>Quasi-convex surface subgroups in some one-relator groups with torsion</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-02 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.01876' target='_blank'>https://doi.org/10.48550/arxiv.2510.01876</a></span></li><li class='work-item'><a href='https://doi.org/10.1016/j.jenvman.2025.126728' target='_blank' class='work-title'>Regional mapping of natural gas compressor stations in the United States and Canada using deep learning on satellite imagery</a><div class='work-meta'>2025 · Journal of Environmental Management · 2025-08-29 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1016/j.jenvman.2025.126728' target='_blank'>https://doi.org/10.1016/j.jenvman.2025.126728</a></span></li><li class='work-item'><a href='https://doi.org/10.1056/aidbp2500144' target='_blank' class='work-title'>MedAgentBench: A Virtual EHR Environment to Benchmark Medical LLM Agents</a><div class='work-meta'>2025 · NEJM AI · 2025-08-14 <span class='oa-badge oa-bronze'>bronze</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1056/aidbp2500144' target='_blank'>https://doi.org/10.1056/aidbp2500144</a></span></li></ul></div><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://ai.stanford.edu/~cbfinn/' target='_blank'>Chelsea Finn</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>robotics</span> <span class='topic-badge'>rl</span> <span class='topic-badge'>meta-learning</span> <span class='topic-badge'>embodied ai</span></div><ul class='works-list'><li class='work-item'><a href='http://arxiv.org/abs/2510.10125' target='_blank' class='work-title'>Ctrl-World: A Controllable Generative World Model for Robot Manipulation</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-11 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.10125' target='_blank'>https://doi.org/10.48550/arxiv.2510.10125</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2510.02263' target='_blank' class='work-title'>RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-02 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.02263' target='_blank'>https://doi.org/10.48550/arxiv.2510.02263</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2507.07986' target='_blank' class='work-title'>EXPO: Stable Reinforcement Learning with Expressive Policies</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-07-10 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2507.07986' target='_blank'>https://doi.org/10.48550/arxiv.2507.07986</a></span></li><li class='work-item'><a href='https://doi.org/10.1126/scirobotics.adt5254' target='_blank' class='work-title'>SRT-H: A hierarchical framework for autonomous surgery via language-conditioned imitation learning</a><div class='work-meta'>2025 · Science Robotics · 2025-07-09 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1126/scirobotics.adt5254' target='_blank'>https://doi.org/10.1126/scirobotics.adt5254</a></span></li><li class='work-item'><a href='https://doi.org/10.15607/rss.2025.xxi.010' target='_blank' class='work-title'>π₀: A Vision-Language-Action Flow Model for General Robot Control</a><div class='work-meta'>2025 · 2025-06-21 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.15607/rss.2025.xxi.010' target='_blank'>https://doi.org/10.15607/rss.2025.xxi.010</a></span></li><li class='work-item'><a href='https://doi.org/10.15607/rss.2025.xxi.012' target='_blank' class='work-title'>FAST: Efficient Action Tokenization for Vision-Language-Action Models</a><div class='work-meta'>2025 · 2025-06-21 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.15607/rss.2025.xxi.012' target='_blank'>https://doi.org/10.15607/rss.2025.xxi.012</a></span></li><li class='work-item'><a href='https://doi.org/10.15607/rss.2025.xxi.071' target='_blank' class='work-title'>Curating Demonstrations using Online Experience</a><div class='work-meta'>2025 · 2025-06-21 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.15607/rss.2025.xxi.071' target='_blank'>https://doi.org/10.15607/rss.2025.xxi.071</a></span></li><li class='work-item'><a href='https://doi.org/10.15607/rss.2025.xxi.017' target='_blank' class='work-title'>Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success</a><div class='work-meta'>2025 · 2025-06-21 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.15607/rss.2025.xxi.017' target='_blank'>https://doi.org/10.15607/rss.2025.xxi.017</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2505.10251' target='_blank' class='work-title'>Hierarchical Surgical Robot Transformer (SRT-H): Imitation Learning for Autonomous Surgery</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-06-10 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.5281/zenodo.15637073' target='_blank'>https://doi.org/10.5281/zenodo.15637073</a></span></li><li class='work-item'><a href='https://doi.org/10.1109/cvpr52734.2025.00166' target='_blank' class='work-title'>CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models</a><div class='work-meta'>2025 · 2025-06-10 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1109/cvpr52734.2025.00166' target='_blank'>https://doi.org/10.1109/cvpr52734.2025.00166</a></span></li></ul></div><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://nlp.stanford.edu/~manning/' target='_blank'>Christopher D. Manning</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>nlp</span> <span class='topic-badge'>llm</span> <span class='topic-badge'>representation learning</span> <span class='topic-badge'>syntax</span> <span class='topic-badge'>semantics</span></div><ul class='works-list'><li class='work-item'><a href='http://arxiv.org/abs/2510.02569' target='_blank' class='work-title'>Transcribe, Translate, or Transliterate: An Investigation of Intermediate Representations in Spoken Language Models</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-02 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.02569' target='_blank'>https://doi.org/10.48550/arxiv.2510.02569</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2510.01171' target='_blank' class='work-title'>Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-01 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.01171' target='_blank'>https://doi.org/10.48550/arxiv.2510.01171</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2510.00219' target='_blank' class='work-title'>Thoughtbubbles: an Unsupervised Method for Parallel Thinking in Latent Space</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-09-30 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.00219' target='_blank'>https://doi.org/10.48550/arxiv.2510.00219</a></span></li><li class='work-item'><a href='https://doi.org/10.1016/j.jim.2025.113970' target='_blank' class='work-title'>CAR binding efficacy and off-target interactions for common CAR detection reagents</a><div class='work-meta'>2025 · Journal of Immunological Methods · 2025-08-28 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1016/j.jim.2025.113970' target='_blank'>https://doi.org/10.1016/j.jim.2025.113970</a></span></li><li class='work-item'><a href='https://doi.org/10.1038/s41562-025-02273-8' target='_blank' class='work-title'>Quantifying large language model usage in scientific papers</a><div class='work-meta'>2025 · Nature Human Behaviour · 2025-08-04 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1038/s41562-025-02273-8' target='_blank'>https://doi.org/10.1038/s41562-025-02273-8</a></span></li><li class='work-item'><a href='https://doi.org/10.1145/3711896.3736801' target='_blank' class='work-title'>The Surprising Victory of NLP: From Philosophy to Agentic Language Models</a><div class='work-meta'>2025 · 2025-08-01 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1145/3711896.3736801' target='_blank'>https://doi.org/10.1145/3711896.3736801</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2507.18103' target='_blank' class='work-title'>A New Pair of GloVes</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-07-24 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2507.18103' target='_blank'>https://doi.org/10.48550/arxiv.2507.18103</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2506.16678' target='_blank' class='work-title'>Mechanisms vs. Outcomes: Probing for Syntax Fails to Explain Performance on Targeted Syntactic Evaluations</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-06-20 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2506.16678' target='_blank'>https://doi.org/10.48550/arxiv.2506.16678</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2506.03989' target='_blank' class='work-title'>Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-06-04 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2506.03989' target='_blank'>https://doi.org/10.48550/arxiv.2506.03989</a></span></li></ul></div><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://cs.stanford.edu/people/chrismre/' target='_blank'>Christopher Ré</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>ml systems</span> <span class='topic-badge'>weak supervision</span> <span class='topic-badge'>foundation models</span></div><ul class='works-list'><li class='work-item'><a href='https://doi.org/10.1109/cvpr52734.2025.00242' target='_blank' class='work-title'>HMAR: Efficient Hierarchical Masked Auto-Regressive Image Generation</a><div class='work-meta'>2025 · 2025-06-10 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1109/cvpr52734.2025.00242' target='_blank'>https://doi.org/10.1109/cvpr52734.2025.00242</a></span></li></ul></div><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://web.stanford.edu/~jurafsky/' target='_blank'>Dan Jurafsky</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>nlp</span> <span class='topic-badge'>speech</span> <span class='topic-badge'>pragmatics</span> <span class='topic-badge'>computational linguistics</span></div><ul class='works-list'><li class='work-item'><a href='https://doi.org/10.1038/s42256-025-01113-8' target='_blank' class='work-title'>Language models cannot reliably distinguish belief from knowledge and fact</a><div class='work-meta'>2025 · Nature Machine Intelligence · 2025-11-03 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1038/s42256-025-01113-8' target='_blank'>https://doi.org/10.1038/s42256-025-01113-8</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2510.12699' target='_blank' class='work-title'>Generation Space Size: Understanding and Calibrating Open-Endedness of LLM Generations</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-14 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.12699' target='_blank'>https://doi.org/10.48550/arxiv.2510.12699</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2510.15951' target='_blank' class='work-title'>Attention to Non-Adopters</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-10 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.15951' target='_blank'>https://doi.org/10.48550/arxiv.2510.15951</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2510.02569' target='_blank' class='work-title'>Transcribe, Translate, or Transliterate: An Investigation of Intermediate Representations in Spoken Language Models</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-02 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.02569' target='_blank'>https://doi.org/10.48550/arxiv.2510.02569</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2510.01395' target='_blank' class='work-title'>Sycophantic AI Decreases Prosocial Intentions and Promotes Dependence</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-01 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.01395' target='_blank'>https://doi.org/10.48550/arxiv.2510.01395</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2509.18750' target='_blank' class='work-title'>False Friends Are Not Foes: Investigating Vocabulary Overlap in Multilingual Language Models</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-09-23 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2509.18750' target='_blank'>https://doi.org/10.48550/arxiv.2509.18750</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2509.07139' target='_blank' class='work-title'>The ML-SUPERB 2.0 Challenge: Towards Inclusive ASR Benchmarking for All Language Varieties</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-09-08 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2509.07139' target='_blank'>https://doi.org/10.48550/arxiv.2509.07139</a></span></li><li class='work-item'><a href='https://doi.org/10.1111/josi.70017' target='_blank' class='work-title'>Racial Disparities in the Discretionary Context of Traffic Stops: How Organizational Practices Shape Institutional Interactions</a><div class='work-meta'>2025 · Journal of Social Issues · 2025-09-01 <span class='oa-badge oa-hybrid'>hybrid</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1111/josi.70017' target='_blank'>https://doi.org/10.1111/josi.70017</a></span></li><li class='work-item'><a href='https://doi.org/10.21437/interspeech.2025-1013' target='_blank' class='work-title'>The ML-SUPERB 2.0 Challenge: Towards Inclusive ASR Benchmarking for All Language Varieties</a><div class='work-meta'>2025 · 2025-08-17 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.21437/interspeech.2025-1013' target='_blank'>https://doi.org/10.21437/interspeech.2025-1013</a></span></li><li class='work-item'><a href='https://doi.org/10.1126/science.adu8449' target='_blank' class='work-title'>Advancing science- and evidence-based AI policy</a><div class='work-meta'>2025 · Science · 2025-07-31 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1126/science.adu8449' target='_blank'>https://doi.org/10.1126/science.adu8449</a></span></li></ul></div><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://dorsa.fyi/' target='_blank'>Dorsa Sadigh</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>robotics</span> <span class='topic-badge'>human-robot interaction</span> <span class='topic-badge'>rl</span> <span class='topic-badge'>safety</span></div><ul class='works-list'><li class='work-item'><a href='http://arxiv.org/abs/2507.07986' target='_blank' class='work-title'>EXPO: Stable Reinforcement Learning with Expressive Policies</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-07-10 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2507.07986' target='_blank'>https://doi.org/10.48550/arxiv.2507.07986</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2506.19212' target='_blank' class='work-title'>Scaffolding Dexterous Manipulation with Vision-Language Models</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-06-24 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2506.19212' target='_blank'>https://doi.org/10.48550/arxiv.2506.19212</a></span></li><li class='work-item'><a href='https://doi.org/10.15607/rss.2025.xxi.155' target='_blank' class='work-title'>Bridging Perception and Action: Spatially-Grounded Mid-Level Representations for Robot Generalization</a><div class='work-meta'>2025 · 2025-06-21 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.15607/rss.2025.xxi.155' target='_blank'>https://doi.org/10.15607/rss.2025.xxi.155</a></span></li><li class='work-item'><a href='https://doi.org/10.15607/rss.2025.xxi.023' target='_blank' class='work-title'>Robot Data Curation with Mutual Information Estimators</a><div class='work-meta'>2025 · 2025-06-21 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.15607/rss.2025.xxi.023' target='_blank'>https://doi.org/10.15607/rss.2025.xxi.023</a></span></li><li class='work-item'><a href='https://doi.org/10.15607/rss.2025.xxi.074' target='_blank' class='work-title'>Unified Video Action Model</a><div class='work-meta'>2025 · 2025-06-21 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.15607/rss.2025.xxi.074' target='_blank'>https://doi.org/10.15607/rss.2025.xxi.074</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2506.12248' target='_blank' class='work-title'>ProVox: Personalization and Proactive Planning for Situated Human-Robot Collaboration</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-06-13 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2506.12248' target='_blank'>https://doi.org/10.48550/arxiv.2506.12248</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2506.01185' target='_blank' class='work-title'>HoMeR: Learning In-the-Wild Mobile Manipulation via Hybrid Imitation and Whole-Body Control</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-06-01 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2506.01185' target='_blank'>https://doi.org/10.48550/arxiv.2506.01185</a></span></li></ul></div><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://profiles.stanford.edu/fei-fei-li' target='_blank'>Fei-Fei Li</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>computer vision</span> <span class='topic-badge'>multimodal</span> <span class='topic-badge'>robotics</span> <span class='topic-badge'>healthcare</span></div><ul class='works-list'><li class='work-item'><a href='http://arxiv.org/abs/2510.18316' target='_blank' class='work-title'>MoMaGen: Generating Demonstrations under Soft and Hard Constraints for Multi-Step Bimanual Mobile Manipulation</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-21 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.18316' target='_blank'>https://doi.org/10.48550/arxiv.2510.18316</a></span></li><li class='work-item'><a href='https://doi.org/10.21203/rs.3.rs-7359180/v1' target='_blank' class='work-title'>EEG-Based Brain-Computer Interface for Robotic Assistance with User Intention Prediction</a><div class='work-meta'>2025 · 2025-09-19 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.21203/rs.3.rs-7359180/v1' target='_blank'>https://doi.org/10.21203/rs.3.rs-7359180/v1</a></span></li><li class='work-item'><a href='https://doi.org/10.1126/science.adu8449' target='_blank' class='work-title'>Advancing science- and evidence-based AI policy</a><div class='work-meta'>2025 · Science · 2025-07-31 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1126/science.adu8449' target='_blank'>https://doi.org/10.1126/science.adu8449</a></span></li><li class='work-item'><a href='https://doi.org/10.1109/cvprw67362.2025.00350' target='_blank' class='work-title'>An Interactive Agent Foundation Model</a><div class='work-meta'>2025 · 2025-06-11 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1109/cvprw67362.2025.00350' target='_blank'>https://doi.org/10.1109/cvprw67362.2025.00350</a></span></li><li class='work-item'><a href='https://doi.org/10.1109/cvpr52734.2025.00802' target='_blank' class='work-title'>Re-thinking Temporal Search for Long-Form Video Understanding</a><div class='work-meta'>2025 · 2025-06-10 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1109/cvpr52734.2025.00802' target='_blank'>https://doi.org/10.1109/cvpr52734.2025.00802</a></span></li><li class='work-item'><a href='https://doi.org/10.1109/cvpr52734.2025.00994' target='_blank' class='work-title'>Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces</a><div class='work-meta'>2025 · 2025-06-10 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1109/cvpr52734.2025.00994' target='_blank'>https://doi.org/10.1109/cvpr52734.2025.00994</a></span></li><li class='work-item'><a href='https://doi.org/10.1109/cvpr52734.2025.00581' target='_blank' class='work-title'>The Language of Motion: Unifying Verbal and Non-verbal Language of 3D Human Motion</a><div class='work-meta'>2025 · 2025-06-10 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1109/cvpr52734.2025.00581' target='_blank'>https://doi.org/10.1109/cvpr52734.2025.00581</a></span></li><li class='work-item'><a href='https://doi.org/10.1038/s41746-025-01730-y' target='_blank' class='work-title'>Foundation versus domain-specific models for left ventricular segmentation on cardiac ultrasound</a><div class='work-meta'>2025 · npj Digital Medicine · 2025-06-06 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1038/s41746-025-01730-y' target='_blank'>https://doi.org/10.1038/s41746-025-01730-y</a></span></li></ul></div><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://cs.stanford.edu/people/jniebles/' target='_blank'>Juan Carlos Niebles</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>computer vision</span> <span class='topic-badge'>video understanding</span> <span class='topic-badge'>embodied ai</span></div><ul class='works-list'><li class='work-item'><a href='https://doi.org/10.1109/cvprw67362.2025.00635' target='_blank' class='work-title'>AdaVid: Adaptive Video-Language Pretraining</a><div class='work-meta'>2025 · 2025-06-11 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1109/cvprw67362.2025.00635' target='_blank'>https://doi.org/10.1109/cvprw67362.2025.00635</a></span></li><li class='work-item'><a href='https://doi.org/10.1109/cvpr52734.2025.02295' target='_blank' class='work-title'>ViUniT: Visual Unit Tests for More Robust Visual Programming</a><div class='work-meta'>2025 · 2025-06-10 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1109/cvpr52734.2025.02295' target='_blank'>https://doi.org/10.1109/cvpr52734.2025.02295</a></span></li><li class='work-item'><a href='https://doi.org/10.1109/cvpr52734.2025.00802' target='_blank' class='work-title'>Re-thinking Temporal Search for Long-Form Video Understanding</a><div class='work-meta'>2025 · 2025-06-10 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1109/cvpr52734.2025.00802' target='_blank'>https://doi.org/10.1109/cvpr52734.2025.00802</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2506.01275' target='_blank' class='work-title'>Contra4: Evaluating Contrastive Cross-Modal Reasoning in Audio, Video, Image, and 3D</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-06-02 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2506.01275' target='_blank'>https://doi.org/10.48550/arxiv.2506.01275</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2506.02298' target='_blank' class='work-title'>LAM SIMULATOR: Advancing Data Generation for Large Action Model Training via Online Exploration and Trajectory Feedback</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-06-02 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2506.02298' target='_blank'>https://doi.org/10.48550/arxiv.2506.02298</a></span></li></ul></div><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://cs.stanford.edu/~jure/' target='_blank'>Jure Leskovec</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>graph learning</span> <span class='topic-badge'>recommendation</span> <span class='topic-badge'>network science</span></div><ul class='works-list'><li class='work-item'><a href='http://arxiv.org/abs/2510.06377' target='_blank' class='work-title'>Relational Transformer: Toward Zero-Shot Foundation Models for Relational Data</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-07 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.06377' target='_blank'>https://doi.org/10.48550/arxiv.2510.06377</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2509.25604' target='_blank' class='work-title'>RFG: Test-Time Scaling for Diffusion Large Language Model Reasoning with Reward-Free Guidance</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-09-29 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2509.25604' target='_blank'>https://doi.org/10.48550/arxiv.2509.25604</a></span></li><li class='work-item'><a href='https://doi.org/10.1101/2025.09.23.678086' target='_blank' class='work-title'>A deep reinforcement learning platform for antibiotic discovery</a><div class='work-meta'>2025 · bioRxiv (Cold Spring Harbor Laboratory) · 2025-09-23 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1101/2025.09.23.678086' target='_blank'>https://doi.org/10.1101/2025.09.23.678086</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2508.11800' target='_blank' class='work-title'>Uncalibrated Reasoning: GRPO Induces Overconfidence for Stochastic Outcomes</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-08-15 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2508.11800' target='_blank'>https://doi.org/10.48550/arxiv.2508.11800</a></span></li><li class='work-item'><a href='https://doi.org/10.1038/s41586-025-09321-3' target='_blank' class='work-title'>Countrywide natural experiment links built environment to physical activity</a><div class='work-meta'>2025 · Nature · 2025-08-13 <span class='oa-badge oa-hybrid'>hybrid</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1038/s41586-025-09321-3' target='_blank'>https://doi.org/10.1038/s41586-025-09321-3</a></span></li><li class='work-item'><a href='https://doi.org/10.1145/3711896.3736558' target='_blank' class='work-title'>Relational Deep Learning: Challenges, Foundations and Next-Generation Architectures</a><div class='work-meta'>2025 · 2025-08-03 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1145/3711896.3736558' target='_blank'>https://doi.org/10.1145/3711896.3736558</a></span></li><li class='work-item'><a href='https://doi.org/10.1145/3711896.3737139' target='_blank' class='work-title'>Surface-based Molecular Design with Multi-modal Flow Matching</a><div class='work-meta'>2025 · 2025-08-03 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1145/3711896.3737139' target='_blank'>https://doi.org/10.1145/3711896.3737139</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2507.16991' target='_blank' class='work-title'>PyG 2.0: Scalable Learning on Real World Graphs</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-07-22 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2507.16991' target='_blank'>https://doi.org/10.48550/arxiv.2507.16991</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2507.03041' target='_blank' class='work-title'>Optimas: Optimizing Compound AI Systems with Globally Aligned Local Rewards</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-07-03 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2507.03041' target='_blank'>https://doi.org/10.48550/arxiv.2507.03041</a></span></li><li class='work-item'><a href='https://doi.org/10.1101/2025.06.26.661135' target='_blank' class='work-title'>Predicting cellular responses to perturbation across diverse contexts with State</a><div class='work-meta'>2025 · bioRxiv (Cold Spring Harbor Laboratory) · 2025-06-27 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1101/2025.06.26.661135' target='_blank'>https://doi.org/10.1101/2025.06.26.661135</a></span></li></ul></div><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://cs.stanford.edu/~pliang/' target='_blank'>Percy Liang</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>foundation models</span> <span class='topic-badge'>llm</span> <span class='topic-badge'>alignment</span> <span class='topic-badge'>evaluation</span> <span class='topic-badge'>program synthesis</span></div><ul class='works-list'><li class='work-item'><a href='http://arxiv.org/abs/2510.11977' target='_blank' class='work-title'>Holistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-13 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.11977' target='_blank'>https://doi.org/10.48550/arxiv.2510.11977</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2510.07307' target='_blank' class='work-title'>MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-08 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.07307' target='_blank'>https://doi.org/10.48550/arxiv.2510.07307</a></span></li><li class='work-item'><a href='https://doi.org/10.58530/2025/3377' target='_blank' class='work-title'>Using Large Language Models and Retrieval-Augmented Generation in MRI Protocol Selection: Balancing Accuracy and Privacy</a><div class='work-meta'>2025 · Proceedings on CD-ROM - International Society for Magnetic Resonance in Medicine. Scientific Meeting and Exhibition/Proceedings of the International Society for Magnetic Resonance in Medicine, Scientific Meeting and Exhibition · 2025-09-16 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.58530/2025/3377' target='_blank'>https://doi.org/10.58530/2025/3377</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2509.05627' target='_blank' class='work-title'>Audits Under Resource, Data, and Access Constraints: Scaling Laws For Less Discriminatory Alternatives</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-09-06 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2509.05627' target='_blank'>https://doi.org/10.48550/arxiv.2509.05627</a></span></li><li class='work-item'><a href='https://doi.org/10.1126/science.adu8449' target='_blank' class='work-title'>Advancing science- and evidence-based AI policy</a><div class='work-meta'>2025 · Science · 2025-07-31 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1126/science.adu8449' target='_blank'>https://doi.org/10.1126/science.adu8449</a></span></li><li class='work-item'><a href='https://doi.org/10.15607/rss.2025.xxi.017' target='_blank' class='work-title'>Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success</a><div class='work-meta'>2025 · 2025-06-21 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.15607/rss.2025.xxi.017' target='_blank'>https://doi.org/10.15607/rss.2025.xxi.017</a></span></li></ul></div><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://thashim.github.io/' target='_blank'>Tatsunori Hashimoto</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>llm</span> <span class='topic-badge'>evaluation</span> <span class='topic-badge'>robustness</span> <span class='topic-badge'>alignment</span> <span class='topic-badge'>statistics</span></div><ul class='works-list'><li class='work-item'><a href='https://doi.org/10.1093/oxfordhb/9780198940272.013.0021' target='_blank' class='work-title'>Liability for AI-Generated Speech</a><div class='work-meta'>2025 · Oxford University Press eBooks · 2025-07-22 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1093/oxfordhb/9780198940272.013.0021' target='_blank'>https://doi.org/10.1093/oxfordhb/9780198940272.013.0021</a></span></li><li class='work-item'><a href='https://doi.org/10.1038/s42256-025-01038-2' target='_blank' class='work-title'>The future of open human feedback</a><div class='work-meta'>2025 · Nature Machine Intelligence · 2025-06-20 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1038/s42256-025-01038-2' target='_blank'>https://doi.org/10.1038/s42256-025-01038-2</a></span></li></ul></div></div></section><section class='lab-section' id='lab-Stanford_NLP_Group' data-lab='Stanford_NLP_Group'><h2 class='lab-title'>Stanford NLP Group</h2><div class='lab-people-grid'><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://profiles.stanford.edu/christopher-potts' target='_blank'>Chris Potts</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>nlp</span> <span class='topic-badge'>semantics</span> <span class='topic-badge'>pragmatics</span> <span class='topic-badge'>safety</span></div><ul class='works-list'><li class='work-item'><a href='https://doi.org/10.1016/j.envpol.2025.127228' target='_blank' class='work-title'>Polystyrene and polyethylene terephthalate nanoplastics differentially impact mouse ovarian follicle function</a><div class='work-meta'>2025 · Environmental Pollution · 2025-10-08 <span class='oa-badge oa-hybrid'>hybrid</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1016/j.envpol.2025.127228' target='_blank'>https://doi.org/10.1016/j.envpol.2025.127228</a></span></li></ul></div><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://nlp.stanford.edu/~manning/' target='_blank'>Christopher D. Manning</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>nlp</span> <span class='topic-badge'>llm</span> <span class='topic-badge'>representation learning</span> <span class='topic-badge'>syntax</span> <span class='topic-badge'>semantics</span></div><ul class='works-list'><li class='work-item'><a href='http://arxiv.org/abs/2510.02569' target='_blank' class='work-title'>Transcribe, Translate, or Transliterate: An Investigation of Intermediate Representations in Spoken Language Models</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-02 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.02569' target='_blank'>https://doi.org/10.48550/arxiv.2510.02569</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2510.01171' target='_blank' class='work-title'>Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-01 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.01171' target='_blank'>https://doi.org/10.48550/arxiv.2510.01171</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2510.00219' target='_blank' class='work-title'>Thoughtbubbles: an Unsupervised Method for Parallel Thinking in Latent Space</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-09-30 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.00219' target='_blank'>https://doi.org/10.48550/arxiv.2510.00219</a></span></li><li class='work-item'><a href='https://doi.org/10.1016/j.jim.2025.113970' target='_blank' class='work-title'>CAR binding efficacy and off-target interactions for common CAR detection reagents</a><div class='work-meta'>2025 · Journal of Immunological Methods · 2025-08-28 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1016/j.jim.2025.113970' target='_blank'>https://doi.org/10.1016/j.jim.2025.113970</a></span></li><li class='work-item'><a href='https://doi.org/10.1038/s41562-025-02273-8' target='_blank' class='work-title'>Quantifying large language model usage in scientific papers</a><div class='work-meta'>2025 · Nature Human Behaviour · 2025-08-04 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1038/s41562-025-02273-8' target='_blank'>https://doi.org/10.1038/s41562-025-02273-8</a></span></li><li class='work-item'><a href='https://doi.org/10.1145/3711896.3736801' target='_blank' class='work-title'>The Surprising Victory of NLP: From Philosophy to Agentic Language Models</a><div class='work-meta'>2025 · 2025-08-01 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1145/3711896.3736801' target='_blank'>https://doi.org/10.1145/3711896.3736801</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2507.18103' target='_blank' class='work-title'>A New Pair of GloVes</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-07-24 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2507.18103' target='_blank'>https://doi.org/10.48550/arxiv.2507.18103</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2506.16678' target='_blank' class='work-title'>Mechanisms vs. Outcomes: Probing for Syntax Fails to Explain Performance on Targeted Syntactic Evaluations</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-06-20 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2506.16678' target='_blank'>https://doi.org/10.48550/arxiv.2506.16678</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2506.03989' target='_blank' class='work-title'>Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-06-04 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2506.03989' target='_blank'>https://doi.org/10.48550/arxiv.2506.03989</a></span></li></ul></div><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://web.stanford.edu/~jurafsky/' target='_blank'>Dan Jurafsky</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>nlp</span> <span class='topic-badge'>speech</span> <span class='topic-badge'>pragmatics</span> <span class='topic-badge'>computational linguistics</span></div><ul class='works-list'><li class='work-item'><a href='https://doi.org/10.1038/s42256-025-01113-8' target='_blank' class='work-title'>Language models cannot reliably distinguish belief from knowledge and fact</a><div class='work-meta'>2025 · Nature Machine Intelligence · 2025-11-03 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1038/s42256-025-01113-8' target='_blank'>https://doi.org/10.1038/s42256-025-01113-8</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2510.12699' target='_blank' class='work-title'>Generation Space Size: Understanding and Calibrating Open-Endedness of LLM Generations</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-14 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.12699' target='_blank'>https://doi.org/10.48550/arxiv.2510.12699</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2510.15951' target='_blank' class='work-title'>Attention to Non-Adopters</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-10 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.15951' target='_blank'>https://doi.org/10.48550/arxiv.2510.15951</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2510.02569' target='_blank' class='work-title'>Transcribe, Translate, or Transliterate: An Investigation of Intermediate Representations in Spoken Language Models</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-02 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.02569' target='_blank'>https://doi.org/10.48550/arxiv.2510.02569</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2510.01395' target='_blank' class='work-title'>Sycophantic AI Decreases Prosocial Intentions and Promotes Dependence</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-01 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.01395' target='_blank'>https://doi.org/10.48550/arxiv.2510.01395</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2509.18750' target='_blank' class='work-title'>False Friends Are Not Foes: Investigating Vocabulary Overlap in Multilingual Language Models</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-09-23 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2509.18750' target='_blank'>https://doi.org/10.48550/arxiv.2509.18750</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2509.07139' target='_blank' class='work-title'>The ML-SUPERB 2.0 Challenge: Towards Inclusive ASR Benchmarking for All Language Varieties</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-09-08 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2509.07139' target='_blank'>https://doi.org/10.48550/arxiv.2509.07139</a></span></li><li class='work-item'><a href='https://doi.org/10.1111/josi.70017' target='_blank' class='work-title'>Racial Disparities in the Discretionary Context of Traffic Stops: How Organizational Practices Shape Institutional Interactions</a><div class='work-meta'>2025 · Journal of Social Issues · 2025-09-01 <span class='oa-badge oa-hybrid'>hybrid</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1111/josi.70017' target='_blank'>https://doi.org/10.1111/josi.70017</a></span></li><li class='work-item'><a href='https://doi.org/10.21437/interspeech.2025-1013' target='_blank' class='work-title'>The ML-SUPERB 2.0 Challenge: Towards Inclusive ASR Benchmarking for All Language Varieties</a><div class='work-meta'>2025 · 2025-08-17 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.21437/interspeech.2025-1013' target='_blank'>https://doi.org/10.21437/interspeech.2025-1013</a></span></li><li class='work-item'><a href='https://doi.org/10.1126/science.adu8449' target='_blank' class='work-title'>Advancing science- and evidence-based AI policy</a><div class='work-meta'>2025 · Science · 2025-07-31 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1126/science.adu8449' target='_blank'>https://doi.org/10.1126/science.adu8449</a></span></li></ul></div><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://cs.stanford.edu/~diyiy/' target='_blank'>Diyi Yang</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>nlp</span> <span class='topic-badge'>social nlp</span> <span class='topic-badge'>evaluation</span> <span class='topic-badge'>safety</span></div><ul class='works-list'><li class='work-item'><a href='http://arxiv.org/abs/2510.27672' target='_blank' class='work-title'>Culture Cartography: Mapping the Landscape of Cultural Knowledge</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-31 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.27672' target='_blank'>https://doi.org/10.48550/arxiv.2510.27672</a></span></li><li class='work-item'><a href='https://doi.org/10.5210/fm.v30i8.13877' target='_blank' class='work-title'>Understanding #vent channels on Discord</a><div class='work-meta'>2025 · First Monday · 2025-08-17 <span class='oa-badge oa-diamond'>diamond</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.5210/fm.v30i8.13877' target='_blank'>https://doi.org/10.5210/fm.v30i8.13877</a></span></li><li class='work-item'><a href='https://doi.org/10.1038/s41562-025-02273-8' target='_blank' class='work-title'>Quantifying large language model usage in scientific papers</a><div class='work-meta'>2025 · Nature Human Behaviour · 2025-08-04 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1038/s41562-025-02273-8' target='_blank'>https://doi.org/10.1038/s41562-025-02273-8</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2507.12705' target='_blank' class='work-title'>AudioJudge: Understanding What Works in Large Audio Model Based Speech Evaluation</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-07-17 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2507.12705' target='_blank'>https://doi.org/10.48550/arxiv.2507.12705</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2507.03041' target='_blank' class='work-title'>Optimas: Optimizing Compound AI Systems with Globally Aligned Local Rewards</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-07-03 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2507.03041' target='_blank'>https://doi.org/10.48550/arxiv.2507.03041</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2506.20803' target='_blank' class='work-title'>The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-06-25 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2506.20803' target='_blank'>https://doi.org/10.48550/arxiv.2506.20803</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2506.12605' target='_blank' class='work-title'>The Rise of AI Companions: How Human-Chatbot Relationships Influence Well-Being</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-06-14 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2506.12605' target='_blank'>https://doi.org/10.48550/arxiv.2506.12605</a></span></li></ul></div><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://cs.stanford.edu/~pliang/' target='_blank'>Percy Liang</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>foundation models</span> <span class='topic-badge'>llm</span> <span class='topic-badge'>alignment</span> <span class='topic-badge'>evaluation</span> <span class='topic-badge'>program synthesis</span></div><ul class='works-list'><li class='work-item'><a href='http://arxiv.org/abs/2510.11977' target='_blank' class='work-title'>Holistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-13 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.11977' target='_blank'>https://doi.org/10.48550/arxiv.2510.11977</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2510.07307' target='_blank' class='work-title'>MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-08 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.07307' target='_blank'>https://doi.org/10.48550/arxiv.2510.07307</a></span></li><li class='work-item'><a href='https://doi.org/10.58530/2025/3377' target='_blank' class='work-title'>Using Large Language Models and Retrieval-Augmented Generation in MRI Protocol Selection: Balancing Accuracy and Privacy</a><div class='work-meta'>2025 · Proceedings on CD-ROM - International Society for Magnetic Resonance in Medicine. Scientific Meeting and Exhibition/Proceedings of the International Society for Magnetic Resonance in Medicine, Scientific Meeting and Exhibition · 2025-09-16 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.58530/2025/3377' target='_blank'>https://doi.org/10.58530/2025/3377</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2509.05627' target='_blank' class='work-title'>Audits Under Resource, Data, and Access Constraints: Scaling Laws For Less Discriminatory Alternatives</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-09-06 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2509.05627' target='_blank'>https://doi.org/10.48550/arxiv.2509.05627</a></span></li><li class='work-item'><a href='https://doi.org/10.1126/science.adu8449' target='_blank' class='work-title'>Advancing science- and evidence-based AI policy</a><div class='work-meta'>2025 · Science · 2025-07-31 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1126/science.adu8449' target='_blank'>https://doi.org/10.1126/science.adu8449</a></span></li><li class='work-item'><a href='https://doi.org/10.15607/rss.2025.xxi.017' target='_blank' class='work-title'>Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success</a><div class='work-meta'>2025 · 2025-06-21 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.15607/rss.2025.xxi.017' target='_blank'>https://doi.org/10.15607/rss.2025.xxi.017</a></span></li></ul></div><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://thashim.github.io/' target='_blank'>Tatsunori Hashimoto</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>llm</span> <span class='topic-badge'>evaluation</span> <span class='topic-badge'>robustness</span> <span class='topic-badge'>alignment</span> <span class='topic-badge'>statistics</span></div><ul class='works-list'><li class='work-item'><a href='https://doi.org/10.1093/oxfordhb/9780198940272.013.0021' target='_blank' class='work-title'>Liability for AI-Generated Speech</a><div class='work-meta'>2025 · Oxford University Press eBooks · 2025-07-22 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1093/oxfordhb/9780198940272.013.0021' target='_blank'>https://doi.org/10.1093/oxfordhb/9780198940272.013.0021</a></span></li><li class='work-item'><a href='https://doi.org/10.1038/s42256-025-01038-2' target='_blank' class='work-title'>The future of open human feedback</a><div class='work-meta'>2025 · Nature Machine Intelligence · 2025-06-20 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1038/s42256-025-01038-2' target='_blank'>https://doi.org/10.1038/s42256-025-01038-2</a></span></li></ul></div></div></section><section class='lab-section' id='lab-Vision__amp;_Learning_Lab' data-lab='Vision__amp;_Learning_Lab'><h2 class='lab-title'>Vision &amp; Learning Lab</h2><div class='lab-people-grid'><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://profiles.stanford.edu/fei-fei-li' target='_blank'>Fei-Fei Li</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>computer vision</span> <span class='topic-badge'>multimodal</span> <span class='topic-badge'>robotics</span> <span class='topic-badge'>healthcare</span></div><ul class='works-list'><li class='work-item'><a href='http://arxiv.org/abs/2510.18316' target='_blank' class='work-title'>MoMaGen: Generating Demonstrations under Soft and Hard Constraints for Multi-Step Bimanual Mobile Manipulation</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-10-21 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2510.18316' target='_blank'>https://doi.org/10.48550/arxiv.2510.18316</a></span></li><li class='work-item'><a href='https://doi.org/10.21203/rs.3.rs-7359180/v1' target='_blank' class='work-title'>EEG-Based Brain-Computer Interface for Robotic Assistance with User Intention Prediction</a><div class='work-meta'>2025 · 2025-09-19 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.21203/rs.3.rs-7359180/v1' target='_blank'>https://doi.org/10.21203/rs.3.rs-7359180/v1</a></span></li><li class='work-item'><a href='https://doi.org/10.1126/science.adu8449' target='_blank' class='work-title'>Advancing science- and evidence-based AI policy</a><div class='work-meta'>2025 · Science · 2025-07-31 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1126/science.adu8449' target='_blank'>https://doi.org/10.1126/science.adu8449</a></span></li><li class='work-item'><a href='https://doi.org/10.1109/cvprw67362.2025.00350' target='_blank' class='work-title'>An Interactive Agent Foundation Model</a><div class='work-meta'>2025 · 2025-06-11 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1109/cvprw67362.2025.00350' target='_blank'>https://doi.org/10.1109/cvprw67362.2025.00350</a></span></li><li class='work-item'><a href='https://doi.org/10.1109/cvpr52734.2025.00802' target='_blank' class='work-title'>Re-thinking Temporal Search for Long-Form Video Understanding</a><div class='work-meta'>2025 · 2025-06-10 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1109/cvpr52734.2025.00802' target='_blank'>https://doi.org/10.1109/cvpr52734.2025.00802</a></span></li><li class='work-item'><a href='https://doi.org/10.1109/cvpr52734.2025.00994' target='_blank' class='work-title'>Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces</a><div class='work-meta'>2025 · 2025-06-10 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1109/cvpr52734.2025.00994' target='_blank'>https://doi.org/10.1109/cvpr52734.2025.00994</a></span></li><li class='work-item'><a href='https://doi.org/10.1109/cvpr52734.2025.00581' target='_blank' class='work-title'>The Language of Motion: Unifying Verbal and Non-verbal Language of 3D Human Motion</a><div class='work-meta'>2025 · 2025-06-10 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1109/cvpr52734.2025.00581' target='_blank'>https://doi.org/10.1109/cvpr52734.2025.00581</a></span></li><li class='work-item'><a href='https://doi.org/10.1038/s41746-025-01730-y' target='_blank' class='work-title'>Foundation versus domain-specific models for left ventricular segmentation on cardiac ultrasound</a><div class='work-meta'>2025 · npj Digital Medicine · 2025-06-06 <span class='oa-badge oa-gold'>gold</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1038/s41746-025-01730-y' target='_blank'>https://doi.org/10.1038/s41746-025-01730-y</a></span></li></ul></div><div class='person-card'><div class='person-header'><h3 class='person-name'><a href='https://cs.stanford.edu/people/jniebles/' target='_blank'>Juan Carlos Niebles</a></h3><span class='person-status'>Faculty</span></div><div class='person-topics'><span class='topic-badge'>computer vision</span> <span class='topic-badge'>video understanding</span> <span class='topic-badge'>embodied ai</span></div><ul class='works-list'><li class='work-item'><a href='https://doi.org/10.1109/cvprw67362.2025.00635' target='_blank' class='work-title'>AdaVid: Adaptive Video-Language Pretraining</a><div class='work-meta'>2025 · 2025-06-11 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1109/cvprw67362.2025.00635' target='_blank'>https://doi.org/10.1109/cvprw67362.2025.00635</a></span></li><li class='work-item'><a href='https://doi.org/10.1109/cvpr52734.2025.02295' target='_blank' class='work-title'>ViUniT: Visual Unit Tests for More Robust Visual Programming</a><div class='work-meta'>2025 · 2025-06-10 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1109/cvpr52734.2025.02295' target='_blank'>https://doi.org/10.1109/cvpr52734.2025.02295</a></span></li><li class='work-item'><a href='https://doi.org/10.1109/cvpr52734.2025.00802' target='_blank' class='work-title'>Re-thinking Temporal Search for Long-Form Video Understanding</a><div class='work-meta'>2025 · 2025-06-10 <span class='oa-badge oa-closed'>closed</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.1109/cvpr52734.2025.00802' target='_blank'>https://doi.org/10.1109/cvpr52734.2025.00802</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2506.01275' target='_blank' class='work-title'>Contra4: Evaluating Contrastive Cross-Modal Reasoning in Audio, Video, Image, and 3D</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-06-02 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2506.01275' target='_blank'>https://doi.org/10.48550/arxiv.2506.01275</a></span></li><li class='work-item'><a href='http://arxiv.org/abs/2506.02298' target='_blank' class='work-title'>LAM SIMULATOR: Advancing Data Generation for Large Action Model Training via Online Exploration and Trajectory Feedback</a><div class='work-meta'>2025 · arXiv (Cornell University) · 2025-06-02 <span class='oa-badge oa-green'>green</span></div><span class='doi'>DOI: <a href='https://doi.org/https://doi.org/10.48550/arxiv.2506.02298' target='_blank'>https://doi.org/10.48550/arxiv.2506.02298</a></span></li></ul></div></div></section></main><footer>Generated from OpenAlex via fetch_papers.py · This dashboard is local to your machine.</footer></body></html>