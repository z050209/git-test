name: Daily Job Search

permissions:
  contents: write

on:
  schedule:
    - cron: "0 4 * * *"
  workflow_dispatch:

jobs:
  run-job-search:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout main
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: main

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: Run job crawler
        run: python run.py

      - name: Collect results
        run: |
          mkdir -p results
          find . -maxdepth 2 -type f -name "job_results_*.json" -exec mv {} results/ \;

      - name: Build dashboards
        run: |
          for f in results/job_results_*.json; do
            python build_jobs_dashboard.py --in-json "$f"
          done

      - name: Cleanup old files (>30 days)
        run: |
          find results -name "job_results_*.json" -mtime +30 -delete || true
          find results -name "job_results_*.html" -mtime +30 -delete || true

      - name: Keep newest 30
        run: |
          ls -t results/job_results_*.json | tail -n +31 | xargs -r rm
          ls -t results/job_results_*.html | tail -n +31 | xargs -r rm

      - name: Commit to data-history
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "Daily job scraping + dashboards"
          file_pattern: results/*
          branch: data-history
          create_branch: true
          push_options: '--force-with-lease'
