import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# ========= é€šç”¨è¯·æ±‚è®¾ç½® =========

DEFAULT_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/123.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "en-US,en;q=0.9",
}

session = requests.Session()
session.headers.update(DEFAULT_HEADERS)

# ========= è¿‡æ»¤è§„åˆ™ =========

INCLUDE_KEYWORDS = [
    "research",
    "researcher",
    "scientist",
    "machine learning",
    "deep learning",
    "ai engineer",
    "ml engineer",
    "research engineer",
    "nlp",
    "natural language",
    "llm",
    "multimodal",
    "generative",
    "diffusion",
    "reinforcement learning",
    "rlhf",
    "preference learning",
    "tokenization",
    "foundation model",
    "computer vision",
]

EXCLUDE_KEYWORDS = [
    "lead",
    "manager",
    "head",
    "principal",
    "director",
    "architect",
    "vp",
    "senior vice",
    "consultant",
    "intern",
    "internship",
    "student",
]

# æ’é™¤ä¸­å›½å…¬å¸ / åœ°ç‚¹ç›¸å…³
EXCLUDE_COMPANY_KEYWORDS = [
    "china",
    "beijing",
    "shanghai",
    "shenzhen",
    "alibaba",
    "tencent",
    "bytedance",
    "huawei",
    "byte dance",
]

# ==== æ’é™¤å¼ºåˆ¶ä¸é€‚åˆçš„å²—ä½ ====
EXCLUDE_HARD = [
    "postdoc",
    "post-doctoral",
    "post doctoral",
    "phd only",
    "requires phd",
    "phd required",
    "assistant professor",
    "associate professor",
    "professor",
    "faculty",
    "audio",
    "speech recognition",
    "tts",
    "biomedical",
    "molecular",
    "diagnostics",
    "clinical",
    "healthcare",
    "intern",
    "internship",
]

REMOTE_KEYWORDS = [
    "remote",
    "hybrid",
    "flexible",
    "work from home",
    "wfh",
    "remote-friendly",
    "remote friendly",
]


def is_relevant_job(title, company, location, snippet=""):
    text = " ".join([title or "", company or "", location or "", snippet or ""]).lower()

    # 1ï¸âƒ£ ç¡¬æ’é™¤
    if any(k in text for k in EXCLUDE_HARD):
        return False

    # 2ï¸âƒ£ å¿…é¡»å‘½ä¸­æ–¹å‘å…³é”®å­—
    if not any(k in text for k in INCLUDE_KEYWORDS):
        return False

    # 3ï¸âƒ£ æ’é™¤ç®¡ç†å²—ç­‰
    if any(k in text for k in EXCLUDE_KEYWORDS):
        return False

    # 4ï¸âƒ£ æ’é™¤å›½å†…å…¬å¸
    if any(k in text for k in EXCLUDE_COMPANY_KEYWORDS):
        return False

    return True


def detect_remote(location, snippet=""):
    text = " ".join([location or "", snippet or ""]).lower()
    return any(k in text for k in REMOTE_KEYWORDS)


def safe_get(url, **kwargs):
    """ç»Ÿä¸€ GET å°è£…ï¼šè‡ªåŠ¨å¸¦ headersï¼Œå¤±è´¥æ‰“å° warningã€‚"""
    try:
        resp = session.get(url, timeout=15, **kwargs)
        resp.raise_for_status()
        return resp
    except Exception as e:  # pragma: no cover - ç½‘ç»œå¼‚å¸¸
        print(f"[WARN] è¯·æ±‚å¤±è´¥ {url}: {e}")
        return None


def safe_post(url, json=None, **kwargs):
    """ç»Ÿä¸€ POST å°è£…ï¼šé’ˆå¯¹ NTU/Workday è¿™ç±»æ¥å£ï¼Œå¤±è´¥ä¹Ÿä¸è¦è®©ç¨‹åºå´©ã€‚"""
    try:
        resp = session.post(url, json=json, timeout=15, **kwargs)
        resp.raise_for_status()
        return resp
    except Exception as e:  # pragma: no cover - ç½‘ç»œå¼‚å¸¸
        print(f"[WARN] POSTå¤±è´¥ {url}: {e}")
        return None


# ========= å„ç«™ç‚¹æŠ“å–å‡½æ•° =========


def fetch_mistral_jobs():
    """
    Mistral AI å®˜æ–¹ jobsï¼ˆLever é¡µé¢ï¼‰
    """
    url = "https://jobs.lever.co/mistral"
    resp = safe_get(url)
    if not resp:
        return []

    soup = BeautifulSoup(resp.text, "html.parser")
    jobs = []

    for posting in soup.select("div.posting"):
        title_tag = posting.select_one("h5")
        location_tag = posting.select_one("span.sort-by-location")
        link_tag = posting.select_one("a.posting-btn-submit")

        if not (title_tag and link_tag):
            continue

        title = title_tag.get_text(strip=True)
        location = location_tag.get_text(strip=True) if location_tag else ""
        link = link_tag.get("href")
        company = "Mistral AI"

        if not is_relevant_job(title, company, location):
            continue

        jobs.append(
            {
                "company": company,
                "title": title,
                "location": location,
                "remote": detect_remote(location),
                "link": link,
                "source": "Mistral",
            }
        )

    return jobs


def fetch_aisingapore_jobs():
    """
    AI Singapore å®˜æ–¹ careers é¡µé¢ï¼ŒæŠ“å–è·³è½¬åˆ° NUS çš„å²—ä½ã€‚
    """
    url = "https://aisingapore.org/home/careers/"
    resp = safe_get(url)
    if not resp:
        return []

    soup = BeautifulSoup(resp.text, "html.parser")
    jobs = []

    for h2 in soup.select("h2"):
        a = h2.find("a", href=True)
        if not a:
            continue

        link = a["href"]
        if "careers.nus.edu.sg" not in link:
            continue  # åªè¦çœŸæ­£çš„èŒä½é“¾æ¥

        title = a.get_text(strip=True)
        company = "AI Singapore / NUS"
        location = "Singapore"

        if not is_relevant_job(title, company, location):
            continue

        jobs.append(
            {
                "company": company,
                "title": title,
                "location": location,
                "remote": False,
                "link": link,
                "source": "AI Singapore",
            }
        )

    return jobs


def fetch_astar_jobs():
    """
    A*STAR Job Listing é¡µé¢ï¼Œä½¿ç”¨ç®€å•çš„æ–‡æœ¬è¿‡æ»¤ AI/ML ç›¸å…³å²—ä½ã€‚
    """
    base_url = "https://careers.a-star.edu.sg/JobListing.aspx"
    resp = safe_get(base_url)
    if not resp:
        return []

    soup = BeautifulSoup(resp.text, "html.parser")
    jobs = []

    for a in soup.select("a[href*='JobDetails.aspx']"):
        title = a.get_text(strip=True)
        row_text = a.parent.get_text(" ", strip=True) if a.parent else ""
        location = "Singapore"
        company = "A*STAR"

        if not is_relevant_job(title, company, location, snippet=row_text):
            continue

        jobs.append(
            {
                "company": company,
                "title": title,
                "location": location,
                "remote": False,
                "link": urljoin(base_url, a["href"]),
                "source": "A*STAR",
            }
        )

    return jobs


def fetch_sit_jobs():
    """
    Singapore Institute of Technology èŒä½æœç´¢ï¼ˆResearch Engineerï¼‰
    """
    url = "https://careers.singaporetech.edu.sg/search/?createNewAlert=false&q=Research+Engineer&locationsearch="
    resp = safe_get(url)
    if not resp:
        return []

    soup = BeautifulSoup(resp.text, "html.parser")
    jobs = []

    for a in soup.select("a.jobTitle-link"):
        title = a.get_text(strip=True)
        link = urljoin("https://careers.singaporetech.edu.sg", a["href"])
        company = "Singapore Institute of Technology"
        location = "Singapore"

        if not is_relevant_job(title, company, location):
            continue

        jobs.append(
            {
                "company": company,
                "title": title,
                "location": location,
                "remote": False,
                "link": link,
                "source": "SIT",
            }
        )

    return jobs


def fetch_mycareersfuture_jobs():
    """
    MyCareersFuture: æœç´¢ research engineerï¼ŒæŒ‰æ–°å‘èŒä½æ’åºã€‚
    """
    url = "https://www.mycareersfuture.gov.sg/search?search=research%20engineer&sortBy=new_posting_date"
    resp = safe_get(url)
    if not resp:
        return []

    soup = BeautifulSoup(resp.text, "html.parser")
    jobs = []

    for card in soup.select("a[data-testid='job-card-link']"):
        title = card.get_text(strip=True)
        link = urljoin("https://www.mycareersfuture.gov.sg", card["href"])

        parent = card.find_parent("div") or card

        company_tag = parent.select_one("[data-testid='company-hire-info']")
        location_tag = parent.select_one("[data-testid='job-location']")

        company = company_tag.get_text(strip=True) if company_tag else "Unknown"
        location = location_tag.get_text(strip=True) if location_tag else "Singapore"

        snippet = parent.get_text(" ", strip=True)

        if not is_relevant_job(title, company, location, snippet=snippet):
            continue

        jobs.append(
            {
                "company": company,
                "title": title,
                "location": location,
                "remote": detect_remote(location, snippet),
                "link": link,
                "source": "MyCareersFuture",
            }
        )

    return jobs


def fetch_anthropic_remote_jobs():
    """
    Anthropic å®˜æ–¹ jobs é¡µé¢ï¼ŒæŠ“å– Research / ML ç›¸å…³å²—ä½ï¼ˆéƒ¨åˆ† Remote-Friendlyï¼‰ã€‚
    """
    url = "https://www.anthropic.com/jobs"
    resp = safe_get(url)
    if not resp:
        return []

    soup = BeautifulSoup(resp.text, "html.parser")
    jobs = []

    for h3 in soup.find_all("h3"):
        title = h3.get_text(strip=True)
        if not any(
            k in title.lower()
            for k in ["research", "ml", "machine learning", "scientist", "engineer"]
        ):
            continue

        loc_tag = h3.find_next_sibling()
        location = loc_tag.get_text(strip=True) if loc_tag else ""

        apply_link = None
        for a in h3.find_all_next("a", string=lambda s: s and "Apply Now" in s):
            apply_link = a
            break
        if not apply_link or not apply_link.get("href"):
            continue

        company = "Anthropic"

        if not is_relevant_job(title, company, location):
            continue

        is_remote = detect_remote(location)

        jobs.append(
            {
                "company": company,
                "title": title,
                "location": location,
                "remote": is_remote,
                "link": apply_link["href"],
                "source": "Anthropic",
            }
        )

    return jobs


def fetch_nus_jobs():
    """
    NUS careers portal (SmartRecruiters)
    æŠ“å– Research / AI / ML å²—ä½
    """
    url = "https://careers.nus.edu.sg/careers"
    resp = safe_get(url)
    if not resp:
        return []

    soup = BeautifulSoup(resp.text, "html.parser")
    jobs = []

    # å…¼å®¹ä¸¤ç§ç»“æ„ï¼šclass å’Œ data-automation-id
    for card in soup.select("a.job-title, a[data-automation-id='jobTitle']"):
        title = card.get_text(strip=True)
        link = card.get("href")
        if not link:
            continue
        link = urljoin(url, link)

        if not any(
            k in title.lower()
            for k in [
                "research",
                "scientist",
                "ai",
                "machine",
                "deep",
                "learning",
                "nlp",
                "intelligence",
                "computer vision",
                "engineer",
            ]
        ):
            continue

        company = "NUS"
        location = "Singapore"

        if not is_relevant_job(title, company, location):
            continue

        jobs.append(
            {
                "company": company,
                "title": title,
                "location": location,
                "remote": False,
                "link": link,
                "source": "NUS Careers",
            }
        )

    return jobs


def fetch_ntu_jobs():
    """
    NTU Careers (Workday) â€“ ç”¨ HTML è§£æ job åˆ—è¡¨ï¼Œé¿å…è°ƒ JSON API æŠ¥ 400ã€‚
    å…¥å£é¡µ: https://ntu.wd3.myworkdayjobs.com/en-US/Careers
    """
    url = "https://ntu.wd3.myworkdayjobs.com/en-US/Careers"
    resp = safe_get(url)
    if not resp:
        return []

    soup = BeautifulSoup(resp.text, "html.parser")
    jobs = []

    # Workday çš„èŒä½æ ‡é¢˜ä¸€èˆ¬æ˜¯ <a data-automation-id="jobTitle">
    for a in soup.select("a[data-automation-id='jobTitle']"):
        title = a.get_text(strip=True)
        if not title:
            continue

        link = a.get("href")
        if not link:
            continue
        link = urljoin(url, link)

        # å¾€ä¸Šæ‰¾ä¸€å±‚å¤§ä¸€ç‚¹çš„ job å®¹å™¨ï¼Œæ‹¿ç‚¹ä¸Šä¸‹æ–‡å½“ snippet
        job_container = a.find_parent("div")
        snippet = job_container.get_text(" ", strip=True) if job_container else ""

        company = "NTU"
        location = "Singapore"

        # è¿‡æ»¤ä¸€éï¼Œå°½é‡åªä¿ç•™ AI / ML / Research ç›¸å…³å²—ä½
        if not is_relevant_job(title, company, location, snippet=snippet):
            continue

        jobs.append(
            {
                "company": company,
                "title": title,
                "location": location,
                "remote": detect_remote(location, snippet),
                "link": link,
                "source": "NTU Careers",
            }
        )

    return jobs


def fetch_remoterocketship_jobs():
    """
    RemoteRocketship: AI researcher / AI research scientist è¿œç¨‹å²—ä½ã€‚
    """
    urls = [
        "https://www.remoterocketship.com/jobs/ai-researcher/",
        "https://www.remoterocketship.com/jobs/ai-research-scientist/",
    ]

    jobs = []

    for url in urls:
        resp = safe_get(url)
        if not resp:
            continue

        soup = BeautifulSoup(resp.text, "html.parser")

        for apply_link in soup.find_all("a", string=lambda s: s and "Apply" in s):
            card = apply_link.find_parent("div")
            if not card:
                continue

            text = card.get_text(" ", strip=True)

            title_tag = card.find(["h3", "h2", "strong"])
            title = title_tag.get_text(strip=True) if title_tag else "AI role"

            company_tag = card.find("h4")
            company = company_tag.get_text(strip=True) if company_tag else "Remote company"

            location = ""
            for part in text.split("  "):
                if "Remote" in part:
                    location = part.strip()
                    break
            if not location:
                location = "Remote"

            if not is_relevant_job(title, company, location, snippet=text):
                continue

            if any(k in text.lower() for k in EXCLUDE_COMPANY_KEYWORDS):
                continue

            link = apply_link.get("href")
            if link and link.startswith("/"):
                link = urljoin("https://www.remoterocketship.com", link)

            jobs.append(
                {
                    "company": company,
                    "title": title,
                    "location": location,
                    "remote": True,
                    "link": link or url,
                    "source": "RemoteRocketship",
                }
            )

    return jobs


def fetch_mbzuai_jobs():
    url = "https://mbzuai.ac.ae/careers/"
    resp = safe_get(url)
    if not resp:
        return []
    soup = BeautifulSoup(resp.text, "html.parser")
    jobs = []
    for a in soup.find_all("a", href=True):
        title = a.get_text(strip=True)
        if not title:
            continue
        low = title.lower()

        if not any(
            k in low for k in ["research", "engineer", "scientist", "ai", "machine", "assistant"]
        ):
            continue

        link = urljoin(url, a["href"])
        company = "MBZUAI"
        location = "Abu Dhabi"
        snippet = title

        if not is_relevant_job(title, company, location, snippet):
            continue

        jobs.append(
            {
                "company": company,
                "title": title,
                "location": location,
                "remote": detect_remote(location, snippet),
                "link": link,
                "source": "MBZUAI",
            }
        )
    return jobs


def fetch_ethz_jobs():
    """
    ETH ZÃ¼rich â€“ å®˜æ–¹ jobs.ethz.ch
    å®é™…èŒä½é“¾æ¥æ˜¯ /job/view/xxxï¼Œä¸æ˜¯ /vacancies/
    """
    url = "https://jobs.ethz.ch"
    resp = safe_get(url)
    if not resp:
        return []
    soup = BeautifulSoup(resp.text, "html.parser")
    jobs = []
    # å…³é”®ï¼šåŒ¹é… '/job/view/' è€Œä¸æ˜¯ '/vacancies/'
    for a in soup.select("a[href*='/job/view/']"):
        title = a.get_text(strip=True)
        if not title:
            continue
        low = title.lower()
        if not any(k in low for k in ["research", "engineer", "scientist", "machine", "ai", "deep"]):
            continue

        link = urljoin(url, a["href"])
        company = "ETH ZÃ¼rich"
        location = "Switzerland"
        snippet = title

        if not is_relevant_job(title, company, location, snippet):
            continue

        jobs.append(
            {
                "company": company,
                "title": title,
                "location": location,
                "remote": detect_remote(location),
                "link": link,
                "source": "ETH",
            }
        )
    return jobs


def fetch_epfl_jobs():
    """
    EPFL â€“ Careers é—¨æˆ·æœ‰ cookie/JSï¼Œç®€å• HTML æŠ“ä¸åˆ°æ—¶å°±è¿”å›ç©ºã€‚
    è¿™é‡Œä¿ç•™å‡½æ•°ï¼Œä½†å¾ˆå¯èƒ½æ˜¯ 0 jobsã€‚
    """
    url = "https://careers.epfl.ch/job-search/?keyword=research"
    resp = safe_get(url)
    if not resp:
        return []
    soup = BeautifulSoup(resp.text, "html.parser")
    jobs = []
    # ç®€å•å…œåº•ï¼šå°½é‡åŒ¹é…åŒ…å« research/ai/ml çš„é“¾æ¥
    for a in soup.find_all("a", href=True):
        title = a.get_text(strip=True)
        if not title:
            continue
        low = title.lower()
        if not any(k in low for k in ["research", "engineer", "ml", "ai", "deep"]):
            continue
        link = urljoin(url, a["href"])
        company = "EPFL"
        location = "Switzerland"

        if not is_relevant_job(title, company, location):
            continue

        jobs.append(
            {
                "company": company,
                "title": title,
                "location": location,
                "remote": False,
                "link": link,
                "source": "EPFL",
            }
        )
    return jobs


def fetch_tno_jobs():
    url = "https://www.tno.nl/en/career/vacancies/?q=machine%20learning"
    resp = safe_get(url)
    if not resp:
        return []

    soup = BeautifulSoup(resp.text, "html.parser")
    jobs = []

    for item in soup.select("a[href*='/en/career/vacancies/']"):
        title = item.get_text(strip=True)
        if not title:
            continue

        low = title.lower()

        if not any(k in low for k in ["research", "machine", "ai", "ml", "engineer", "data", "scientist"]):
            continue

        link = urljoin(url, item["href"])
        company = "TNO"
        location = "Netherlands"

        if not is_relevant_job(title, company, location):
            continue

        jobs.append(
            {
                "company": company,
                "title": title,
                "location": location,
                "remote": detect_remote(location),
                "link": link,
                "source": "TNO",
            }
        )

    return jobs


BASE_URL = "https://jobs.fraunhofer.de"
SEARCH_URL = BASE_URL + "/search/"

FRAUNHOFER_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/123.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "de-DE,de;q=0.9,en-US;q=0.8,en;q=0.7",
}

fraunhofer_session = requests.Session()
fraunhofer_session.headers.update(FRAUNHOFER_HEADERS)


def fetch_fraunhofer_jobs():
    """
    æŠ“ https://jobs.fraunhofer.de/search/?q=ai&startrow=0,25,50... ä¸Šçš„æ‰€æœ‰ /job/ é“¾æ¥ï¼Œ
    å†ç”¨ä½ è‡ªå·±çš„ is_relevant_job() è¿‡æ»¤ã€‚
    """
    all_jobs = []
    startrow = 0
    page_size = 25  # é¡µé¢ä¸Šå†™äº† â€œErgebnisse 1 â€“ 25 von ...â€

    while True:
        params = {
            "q": "ai",
            "startrow": startrow,
        }
        print(f"[fraunhofer] startrow={startrow}")
        resp = fraunhofer_session.get(SEARCH_URL, params=params, timeout=15)
        resp.raise_for_status()

        soup = BeautifulSoup(resp.text, "html.parser")

        # 1. æŠ“æ‰€æœ‰ /job/ é“¾æ¥
        links = soup.select("a[href*='/job/']")
        if not links:
            # æ²¡èŒä½äº†ï¼Œæˆ–è€…è¢« cookie é¡µæŒ¡ä½
            break

        new_count = 0

        for a in links:
            href = a.get("href")
            title = a.get_text(strip=True)

            if not href or not title:
                continue

            link = urljoin(BASE_URL, href)
            company = "Fraunhofer"
            location = "Germany"  # å…ˆç®€å•å†™æ­»

            # è¿™é‡Œç”¨ä½ åŸæ¥çš„è¿‡æ»¤é€»è¾‘
            if not is_relevant_job(title, company, location, snippet=title):
                continue

            all_jobs.append(
                {
                    "company": company,
                    "title": title,
                    "location": location,
                    "remote": detect_remote(location, title),
                    "link": link,
                    "source": "Fraunhofer",
                }
            )
            new_count += 1

        print(f"[fraunhofer] è¿™ä¸€é¡µç¬¦åˆæ¡ä»¶çš„èŒä½: {new_count}")

        # åˆ†é¡µï¼šå¦‚æœè¿™ä¸€é¡µçš„ job é“¾æ¥ä¸åˆ° page_size ä¸ªï¼Œè¯´æ˜å·²ç»åˆ°æœ€åä¸€é¡µ
        if len(links) < page_size:
            break

        startrow += page_size

    return all_jobs


def fetch_sintef_jobs():
    """
    SINTEF â€“ Vacant positions é¡µé¢ï¼Œå®é™…èŒä½é“¾æ¥è·³åˆ° delta.hr-manager.netã€‚
    """
    url = "https://www.sintef.no/en/sintef-group/career/vacant-positions/"
    resp = safe_get(url)
    if not resp:
        return []

    soup = BeautifulSoup(resp.text, "html.parser")
    jobs = []

    # å…³é”®ï¼šé“¾æ¥åŸŸåæ˜¯ delta.hr-manager.net
    for a in soup.select("a[href*='delta.hr-manager.net']"):
        title = a.get_text(strip=True)
        if not title:
            continue

        low = title.lower()
        # æ’é™¤â€œæ›´æ–°ç”³è¯·â€ç­‰æ–‡å­—ï¼Œå°½é‡è¦çœŸæ­£èŒä½
        if "update previous applications" in low:
            continue

        if not any(k in low for k in ["research", "engineer", "scientist", "ai", "ml", "deep", "data", "robot"]):
            continue

        link = a.get("href")
        if link and link.startswith("/"):
            link = urljoin(url, link)

        company = "SINTEF"
        location = "Norway"

        if not is_relevant_job(title, company, location):
            continue

        jobs.append(
            {
                "company": company,
                "title": title,
                "location": location,
                "remote": detect_remote(location),
                "link": link,
                "source": "SINTEF",
            }
        )

    return jobs


def fetch_vtt_jobs():
    """
    VTT (Finland) â€“ é‡ç‚¹æŠ“ AI / ML / Robotics / Simulation ç›¸å…³å²—ä½
    """
    url = "https://www.vttresearch.com/en/working-vtt"
    resp = safe_get(url)
    if not resp:
        return []

    soup = BeautifulSoup(resp.text, "html.parser")
    jobs = []

    for a in soup.select("a[href*='/en/careers/open-positions/']"):
        title = a.get_text(strip=True)
        if not title:
            continue

        link = urljoin(url, a.get("href"))
        company = "VTT"
        location = "Finland"

        if not is_relevant_job(title, company, location):
            continue

        jobs.append(
            {
                "company": company,
                "title": title,
                "location": location,
                "remote": detect_remote(location),
                "link": link,
                "source": "VTT",
            }
        )

    return jobs


def fetch_tudelft_jobs():
    """
    TU Delft â€“ æŠ“ Research / Engineer / AI / Robotics ç›¸å…³å²—ä½
    """
    url = "https://www.tudelft.nl/en/about-tu-delft/working-at-tu-delft/search-jobs"
    resp = safe_get(url)
    if not resp:
        return []

    soup = BeautifulSoup(resp.text, "html.parser")
    jobs = []

    for a in soup.select("a[href*='/vacature'], a[href*='/job']"):
        title = a.get_text(strip=True)
        if not title:
            continue

        link = urljoin(url, a.get("href"))
        company = "TU Delft"
        location = "Netherlands"

        if not is_relevant_job(title, company, location):
            continue

        jobs.append(
            {
                "company": company,
                "title": title,
                "location": location,
                "remote": detect_remote(location),
                "link": link,
                "source": "TU Delft",
            }
        )

    return jobs


def fetch_kth_jobs():
    """
    KTH (Sweden) â€“ æŠ“ AI / ML / Robotics / Control ç›¸å…³å²—ä½
    """
    url = "https://www.kth.se/lediga-jobb"
    resp = safe_get(url)
    if not resp:
        return []

    soup = BeautifulSoup(resp.text, "html.parser")
    jobs = []

    for a in soup.select("a[href*='/jobb'], a[href*='/positions']"):
        title = a.get_text(strip=True)
        if not title:
            continue

        link = urljoin(url, a.get("href"))
        company = "KTH"
        location = "Sweden"

        if not is_relevant_job(title, company, location):
            continue

        jobs.append(
            {
                "company": company,
                "title": title,
                "location": location,
                "remote": detect_remote(location),
                "link": link,
                "source": "KTH",
            }
        )

    return jobs


def fetch_dtu_jobs():
    """
    DTU (Denmark) â€“ æŠ“ AI / Robotics / Automation / Control å²—ä½
    """
    url = "https://www.dtu.dk/english/About/JOB-and-CAREER/vacant-positions"
    resp = safe_get(url)
    if not resp:
        return []

    soup = BeautifulSoup(resp.text, "html.parser")
    jobs = []

    for a in soup.select("a[href*='/About/JOB-and-CAREER/vacant-positions']"):
        title = a.get_text(strip=True)
        if not title:
            continue

        link = urljoin(url, a.get("href"))
        company = "DTU"
        location = "Denmark"

        if not is_relevant_job(title, company, location):
            continue

        jobs.append(
            {
                "company": company,
                "title": title,
                "location": location,
                "remote": detect_remote(location),
                "link": link,
                "source": "DTU",
            }
        )

    return jobs


def fetch_stability_jobs():
    """
    Stability AI Careers â€“ æŠ“ Research / Scientist / Engineer ä¸­å’Œç”Ÿæˆå¼ç›¸å…³çš„å²—ä½
    """
    url = "https://stability.ai/careers"
    resp = safe_get(url)
    if not resp:
        return []

    soup = BeautifulSoup(resp.text, "html.parser")
    jobs = []

    for a in soup.select("a[href*='/careers/']"):
        title = a.get_text(strip=True)
        if not title:
            continue

        link = urljoin(url, a.get("href"))
        company = "Stability AI"
        location = "Europe / Remote"

        if not is_relevant_job(title, company, location):
            continue

        jobs.append(
            {
                "company": company,
                "title": title,
                "location": location,
                "remote": True,
                "link": link,
                "source": "Stability",
            }
        )

    return jobs


def fetch_runway_jobs():
    """
    Runway Careers â€“ æŠ“ research / generative / video / ml ç›¸å…³å²—ä½
    """
    url = "https://runwayml.com/careers"
    resp = safe_get(url)
    if not resp:
        return []

    soup = BeautifulSoup(resp.text, "html.parser")
    jobs = []

    for a in soup.select("a[href*='/careers/']"):
        title = a.get_text(strip=True)
        if not title:
            continue

        link = urljoin(url, a.get("href"))
        company = "Runway"
        location = "Europe / Remote"

        if not is_relevant_job(title, company, location):
            continue

        jobs.append(
            {
                "company": company,
                "title": title,
                "location": location,
                "remote": True,
                "link": link,
                "source": "Runway",
            }
        )

    return jobs


def fetch_eleven_jobs():
    """
    ElevenLabs Careers â€“ æŠ“ research / ml / generative / audio-multimodal å²—ä½
    """
    url = "https://elevenlabs.io/careers"
    resp = safe_get(url)
    if not resp:
        return []

    soup = BeautifulSoup(resp.text, "html.parser")
    jobs = []

    for a in soup.select("a[href*='/careers/']"):
        title = a.get_text(strip=True)
        if not title:
            continue

        link = urljoin(url, a.get("href"))
        company = "ElevenLabs"
        location = "Europe / Remote"

        if not is_relevant_job(title, company, location):
            continue

        jobs.append(
            {
                "company": company,
                "title": title,
                "location": location,
                "remote": True,
                "link": link,
                "source": "ElevenLabs",
            }
        )

    return jobs
def fetch_jobscentral_jobs():
    """
    JobsCentral æ–°åŠ å¡å²—ä½ï¼ˆjobscentral.com.sgï¼‰
    é€šè¿‡ ?title= æŸ¥è¯¢è‹¥å¹² AI/ML ç›¸å…³å…³é”®è¯ï¼Œå†é€ä¸ªè¿›è¯¦æƒ…é¡µè¿‡æ»¤ã€‚
    """
    base_list_url = "https://jobscentral.com.sg/jobs"
    base_domain = "https://jobscentral.com.sg"
    search_terms = [
        "research engineer",
        "research scientist",
        "machine learning",
        "deep learning",
        "ai engineer",
        "ml engineer",
        "data scientist",
        "computer vision",
        "nlp",
        "generative",
    ]

    jobs = []
    seen_links = set()

    for term in search_terms:
        query = term.replace(" ", "+")
        list_url = f"{base_list_url}?title={query}&location=Singapore"
        resp = safe_get(list_url)
        if not resp:
            continue

        soup = BeautifulSoup(resp.text, "html.parser")

        # JobsCentral èŒä½è¯¦æƒ…é“¾æ¥å½¢å¦‚ /jobs/other-jobs/1035726
        for a in soup.select("a[href*='/jobs/']"):
            href = a.get("href")
            if not href:
                continue

            # æ’é™¤é¡¶éƒ¨ â€œBrowse jobsâ€ è¿™ç§çŸ­é“¾æ¥ï¼ˆä¾‹å¦‚ "/jobs"ï¼‰
            if href.count("/") <= 2:
                continue

            link = urljoin(base_domain, href)

            # é¿å…é‡å¤
            if link in seen_links:
                continue
            seen_links.add(link)

            # è¿›å…¥è¯¦æƒ…é¡µï¼Œæ‹¿æ›´å¤šæ–‡æœ¬ç”¨äºè¿‡æ»¤
            detail_resp = safe_get(link)
            if not detail_resp:
                continue

            detail_soup = BeautifulSoup(detail_resp.text, "html.parser")

            # æ ‡é¢˜ï¼šä¼˜å…ˆç”¨ <h1>ï¼Œé€€åŒ–åˆ° <title> æˆ–åˆ—è¡¨é¡µä¸Šçš„æ–‡æœ¬
            title_tag = detail_soup.find("h1") or detail_soup.find("h2")
            if title_tag:
                title = title_tag.get_text(strip=True)
            else:
                # å…œåº•ï¼šç”¨è¯¦æƒ…é¡µ title æ ‡ç­¾æˆ–åˆ—è¡¨é¡µ a æ–‡æœ¬
                if detail_soup.title:
                    title = detail_soup.title.get_text(strip=True)
                else:
                    title = a.get_text(strip=True)

            # ä½ç½®ï¼šJobsCentral å¤§éƒ¨åˆ†éƒ½æ˜¯æ–°åŠ å¡å²—ä½
            location = "Singapore"

            # snippetï¼šæ•´ä¸ªé¡µé¢æ–‡æœ¬ï¼Œæ–¹ä¾¿å…³é”®è¯è¿‡æ»¤
            snippet = detail_soup.get_text(" ", strip=True)

            # è¿™é‡Œåªèƒ½ç²—ç•¥ç»™ä¸ª companyï¼ŒçœŸæ­£å…¬å¸åç»“æ„æ¯”è¾ƒæ•£ï¼Œå°±å…ˆä¸å¼ºè¡Œè§£æ
            company = "JobsCentral listing"

            if not is_relevant_job(title, company, location, snippet=snippet):
                continue

            jobs.append(
                {
                    "company": company,
                    "title": title,
                    "location": location,
                    "remote": detect_remote(location, snippet),
                    "link": link,
                    "source": "JobsCentral",
                }
            )

    return jobs
def fetch_mycareersfuture_jobs():
    """
    MyCareersFuture (CareersFuture Job Portal):
    éå†å¤šç§ AI/ML å…³é”®è¯ï¼ŒæŒ‰æ–°å‘èŒä½æ’åºã€‚
    """
    base_url = "https://www.mycareersfuture.gov.sg/search"
    query_terms = [
        "research engineer",
        "research scientist",
        "machine learning",
        "deep learning",
        "ai engineer",
        "ml engineer",
        "data scientist",
        "nlp",
        "computer vision",
        "generative",
    ]

    jobs = []
    seen_links = set()

    for term in query_terms:
        search_param = term.replace(" ", "%20")
        url = f"{base_url}?search={search_param}&sortBy=new_posting_date"
        resp = safe_get(url)
        if not resp:
            continue

        soup = BeautifulSoup(resp.text, "html.parser")

        for card in soup.select("a[data-testid='job-card-link']"):
            title = card.get_text(strip=True)
            link = urljoin("https://www.mycareersfuture.gov.sg", card["href"])

            if link in seen_links:
                continue
            seen_links.add(link)

            parent = card.find_parent("div") or card
            company_tag = parent.select_one("[data-testid='company-hire-info']")
            location_tag = parent.select_one("[data-testid='job-location']")

            company = company_tag.get_text(strip=True) if company_tag else "Unknown"
            location = location_tag.get_text(strip=True) if location_tag else "Singapore"

            snippet = parent.get_text(" ", strip=True)

            if not is_relevant_job(title, company, location, snippet=snippet):
                continue

            jobs.append(
                {
                    "company": company,
                    "title": title,
                    "location": location,
                    "remote": detect_remote(location, snippet),
                    "link": link,
                    "source": "MyCareersFuture",
                }
            )

    return jobs


# ========= æ±‡æ€» & æ‰“å° =========


def collect_all_jobs():
    all_jobs = []

    fetchers = [
        # ğŸ‡ªğŸ‡º æ ¸å¿ƒæ¬§æ´²ç§‘ç ”
        fetch_tno_jobs,
        fetch_fraunhofer_jobs,
        fetch_sintef_jobs,
        fetch_ethz_jobs,
        fetch_epfl_jobs,
        fetch_vtt_jobs,
        fetch_tudelft_jobs,
        fetch_kth_jobs,
        fetch_dtu_jobs,
        # ğŸŒ å‰æ²¿ç”Ÿæˆå¼ AIï¼Œä¸æ˜¯ç¾ä¼
        fetch_mistral_jobs,
        fetch_stability_jobs,
        fetch_runway_jobs,
        fetch_eleven_jobs,
        # ğŸ‡¸ğŸ‡¬ æ–°åŠ å¡ç§‘ç ”
        fetch_aisingapore_jobs,
        fetch_astar_jobs,
        fetch_nus_jobs,
        fetch_ntu_jobs,
        fetch_sit_jobs,
        fetch_mycareersfuture_jobs,  # CareersFuture job portal
        fetch_jobscentral_jobs,      # JobsCentral æ–°å¢
        # Remote æ¬§æ´²å²— / æµ·å¤–ç§‘ç ”
        # Remote æ¬§æ´²å²—
        fetch_remoterocketship_jobs,
        fetch_mbzuai_jobs,
    ]

    for f in fetchers:
        print(f"[INFO] Fetching from {f.__name__} ...")
        try:
            jobs = f()
            print(f"[INFO]   -> {len(jobs)} jobs")
            all_jobs.extend(jobs)
        except Exception as e:  # pragma: no cover - ç½‘ç»œå¼‚å¸¸
            print(f"[ERROR] {f.__name__} å‘ç”Ÿå¼‚å¸¸: {e}")

    # æŒ‰é“¾æ¥å»é‡
    seen = set()
    unique_jobs = []
    for j in all_jobs:
        key = j["link"]
        if key not in seen:
            seen.add(key)
            unique_jobs.append(j)

    return unique_jobs


def score_job(job):
    score = 0

    title = job.get("title", "").lower()
    snippet = job.get("snippet", "").lower()
    company = job.get("company", "").lower()
    location = job.get("location", "").lower()

    # ===== ä½ æ ¸å¿ƒèƒ½åŠ›ï¼ˆé«˜æƒé‡ï¼‰ =====
    high = [
        ("multimodal", 10),
        ("multi-modal", 10),
        ("large language model", 9),
        ("foundation", 8),
        ("agent", 8),
        ("rlhf", 8),
        ("reinforcement", 8),
        ("generative", 8),
        ("diffusion", 7),
        ("vae", 6),
        ("vq", 6),
        ("token", 6),
        ("tokenization", 6),
        ("motion", 6),
        ("bvh", 5),
        ("3d", 5),
    ]
    for kw, weight in high:
        if kw in title or kw in snippet:
            score += weight

    # ===== robotics / control =====
    robotics = [
        ("robot", 6),
        ("embodied", 6),
        ("simulation", 5),
        ("control", 5),
        ("autonomous", 4),
    ]
    for kw, weight in robotics:
        if kw in title or kw in snippet:
            score += weight

    # ===== research engineerç³» =====
    if "research engineer" in title:
        score += 10
    if "research scientist" in title:
        score += 8
    if "applied scientist" in title:
        score += 8

    # generic engineer åŠ ä¸€ç‚¹
    if "engineer" in title:
        score += 3

    # ===== åœ°ç†åå¥½ =====
    if "remote" in location:
        score += 10
    if any(k in location for k in ["netherlands", "norway", "finland", "sweden", "germany", "switzerland"]):
        score += 7
    if "singapore" in location:
        score += 5
    if "us" in location:
        score -= 1

    # ===== ç ”ç©¶æœºæ„åå¥½ =====
    preferred = {
        "tno": 8,
        "fraunhofer": 7,
        "sintef": 7,
        "eth": 7,
        "epfl": 6,
        "mistral": 9,
        "runway": 8,
        "stability": 8,
        "deepmind": 7,
        "microsoft research": 6,
        "aisingapore": 5,
        "astar": 5,
    }
    for keyword, weight in preferred.items():
        if keyword in company:
            score += weight

    # ===== é»‘åå• =====
    blacklist = ["huawei", "alibaba", "tencent", "bytedance", "sensetime"]
    if any(k in company for k in blacklist):
        return -100

    return score


def print_jobs(jobs):
    if not jobs:
        print("ä»Šå¤©æ²¡æœ‰æŠ“åˆ°ç¬¦åˆæ¡ä»¶çš„èŒä½ã€‚")
        return

    for i, job in enumerate(jobs, 1):
        remote_flag = " ğŸŒREMOTE" if job.get("remote") else ""
        print(f"#{i} {job['company']} â€” {job['title']}{remote_flag}")
        print(f"   â­ Score: {job.get('score', 0)}")
        print(f"   ğŸ“ {job['location']} | æ¥æº: {job['source']}")
        print(f"   ğŸ”— {job['link']}")
        print("-" * 60)


def main():
    print("å¼€å§‹æŠ“å–èŒä½...\n")
    jobs = collect_all_jobs()
    for job in jobs:
        job["score"] = score_job(job)
    sorted_jobs = sorted(jobs, key=lambda x: x["score"], reverse=True)
    print_jobs(sorted_jobs)
    return sorted_jobs


if __name__ == "__main__":
    main()
